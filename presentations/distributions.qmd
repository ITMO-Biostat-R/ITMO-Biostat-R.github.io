---
title: "Distributions"
format: 
  revealjs:
      center: true
      controls: true
      # controlsLayout: 'bottom-left'
      slide-number: true
      show-slide-number: all
      fontsize: 30px
      style: style.css
      progress: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.width = 8, fig.height = 4)
library(dplyr)
library(tidyr)
library(ggplot2)
```

## Types of Data

-   **Numerical data**: Continuous (e.g., height) or discrete (e.g., number of leaves).
-   **Categorical data**: Nominal (e.g., species, gender) or ordinal (e.g., rankings).

![](/images/DataTypes.png){fig-align="center" width="1800"}

## Sampling {auto-animate="true"}

Usually we **cannot take the whole population** for a study.

> For example, it is physically impossible to do a study using blood samples from all the people in the world. Sometimes the population is small, but it is extremely expensive to collect the material.

Therefore, researchers use samples - **random and representative** groups taken from the general population that can be analysed to **provide information about the whole population.**

### This is where statistics come to the rescue.

## Sampling {auto-animate="true"}

### Are these samples are representative?

1.  20 ITMO Biology students to study average marks of students in ITMO
2.  100 ITMO first-year students to study students workload in their first ever exam
3.  Email survey on student parents' opinion of ITMO
4.  The double-blind, randomized, placebo-controlled trial in 1990

------------------------------------------------------------------------

1.  20 ITMO Biology students to study average marks of students in ITMO
    -   **Representive only for Biology students**
2.  100 ITMO first-year students to study students workload
    -   **Represetive only for ITMO. Students with high workloads are less likely to participate**
3.  Email survey on student parents' opinion of ITMO
    -   **Only people whose email addresses we have**
4.  The double-blind, randomized, placebo-controlled trial in 1990
    -   **Too old**

------------------------------------------------------------------------

### [Results of the student survey, 2024 (449 people)](https://fitp.itmo.ru/p/abit/804)

::::: columns
::: {.column width="45%"}
![](/images/memes/itmosurvey.png){fig-align="center" width="1000"}
:::

::: {.column width="55%"}
![](/images/memes/itmopie.png){fig-align="center"} ![](/images/memes/itmolegend.png){fig-align="center"}
:::
:::::

------------------------------------------------------------------------

::::: columns
::: {.column width="50%"}
![](/images/memes/roulette.jpg){fig-align="center" width="500"}
:::

::: {.column width="50%"}
![](/images/memes/selection_bias.png){fig-align="center" width="500"}
:::
:::::

## Sampling {auto-animate="true"}

![](../images/sampling_types.jpg){fig-align="center" width="800"}

## Sampling {auto-animate="true"}

::::: columns
::: {.column width="50%"}
```{r, fig.height=5, fig.width=6}
normgenpop = rnorm(100000) %>%  as.data.frame(nm = 'x')
normgenpop %>% ggplot(aes(x=x)) +
geom_density(col='black', fill="lightblue") +
theme_classic() + labs(title='General population (100000 objects)') +
    theme(title = element_text(size=20))

# normgenpop = rnorm(100000) 
# hist(normgenpop, br=50, xlab='X', main = 'General population')
```
:::

::: {.column .fragment .fade-in width="50%"}
```{r, fig.height=5, fig.width=6}
ggplot() + 
  geom_density(data=sample_n(normgenpop, 10), fill='lightblue1',
                 aes(x=x), alpha=0.4) +
  geom_density(data=sample_n(normgenpop, 10),fill='green', 
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 10), fill='coral',
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 10), fill = 'violet',
                 aes(x=x), alpha=0.4) +
  theme_classic() + labs(title='4 samples of 10 objects') +
  theme(title = element_text(size=20))
```
:::
:::::

------------------------------------------------------------------------

### The larger the sample, the more similar it is to the general population

```{r,}
ggplot() + 
  geom_density(data=sample_n(normgenpop, 1000), fill='lightblue1',
                 aes(x=x), alpha=0.4) +
  geom_density(data=sample_n(normgenpop, 1000),fill='green', 
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 1000), fill='coral',
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 1000), fill = 'violet',
                 aes(x=x), alpha=0.4) +
  theme_classic() + labs(title='4 samples of 1000 objects') +
  theme(title = element_text(size=20))
```

## Descriptive sample statistics {auto-animate="true"}

### Central tendency

-   **Mean** - $\bar{X} = \frac{x_1 + x_2 + ... x_N}{N}$

```{r, eval=F, echo=T, class="large-code"}
mean(x)
```

-   **Median** - the $N/2$-th element in list of sorted values $\{x_1, x_2, ..., \pmb{x_{N/2}},...,x_{N-1}, x_N\}$, where for any $i$: $x_{i-1} < x_i$

```{r, eval=F, echo=T, class="large-code"}
median(x)
```

-   **Mode** - The most frequent value *one of the methods:*\
    *only for discrete data:*

```{r, eval=F, echo=T, class="large-code"}
x %>% table() %>% sort(decreasing = T) %>% head(1) %>% names() %>% as.numeric()
```

```{r, eval=T, echo=F, class="large-code"}
mode_fun = function(x) x %>% table() %>% sort(decreasing = T) %>% head(1) %>% names() %>% as.numeric()
```

***They are equal for a normal distribution***

## Descriptive sample statistics {auto-animate="true"}

### Bimodal data

```{r, eval=T, echo=F,  fig.width = 8, fig.height = 5}
twomodes = c(rbinom(10000, 31, 0.4), rbinom(10000, 31, 0.7))

hist(twomodes, breaks=30, 
     main=NULL,
     xlab='X')
abline(v = 30*0.4, lwd=3, col='red')
abline(v = 31*0.7, lwd=3, col='red')
abline(v = mean(twomodes), lwd=3, col='blue')
abline(v = median(twomodes), lwd=3, col='green3')

legend("topright",
       col = c('red', 'blue', 'green3'), 
       legend = c('modes', 'mean', "median"), 
        lwd = 3, bty = "n")

```

## Descriptive sample statistics {auto-animate="true"}

### Exponential distribution

```{r, eval=T, echo=F,  fig.width = 7, fig.height = 4}
expsmall = rexp(10000, 10)

hist(expsmall, breaks=30, 
     main=NULL,
     xlab='X')
abline(v = mode_fun(expsmall), lwd=3, col='red')
abline(v = mean(expsmall), lwd=3, col='blue')
abline(v = median(expsmall), lwd=3, col='green3')

legend("topright",
       col = c('red', 'blue', 'green3'), 
       legend = c('mode', 'mean', "median"), 
        lwd = 3, bty = "n")

```

## Descriptive sample statistics {auto-animate="true"}

### Outliers

**Mean is more sensitive to outliers**

```{r, eval=T, echo=F,  fig.width = 8, fig.height = 4}
normoutlier = c(rnorm(100), 50, 50, 50)

hist(normoutlier, breaks=60, 
     main=NULL,
     xlab='X')
# abline(v = 0, lwd=3, col='red')
abline(v = mean(normoutlier), lwd=3, col='blue')
abline(v = median(normoutlier), lwd=3, col='green3')

# text(x=7, y=10, label='mean')

legend("topright",
       col = c(
         # 'red',
         'blue', 'green3'), 
       legend = c(
         # 'mode',
         'mean', "median"), 
        lwd = 3, bty = "n")

```

## By the way, what to do about outliers?

**Nothing**

-   If they still contain some information.Â 

**Delete**

-   If there are few of them.
-   This is an obvious error and [**you are sure**]{.underline} they do not contain valuable information

## Descriptive sample statistics {auto-animate="true"}

### Variance and Standard deviation

$Var(X) = S^2 = \dfrac{((x_1 - \bar{X})^2 + (x_2 - \bar{X})^2 + ... + (x_N - \bar{X})^2)}{N-1} = \dfrac{\sum{(x_i - \bar{X})^2}}{N-1}$

```{r, eval=F, echo=T, class="large-code"}
var(x)
```

$SD(X) = S = \sqrt{\dfrac{\sum{(x_i - \bar{X})^2}}{N-1}}$

```{r, eval=F, echo=T, class="large-code"}
sd(x)
```

> For the **population** variance in the denominator of the formula is $N$, not $N-1$

## Descriptive sample statistics {auto-animate="true"}

### Quantiles

**Quantiles** - values less than which $X$% of the data in the distribution is found - For example, 0.75 quntile is the value that is greater of 75% of values in sample, but lower than another 25% of sample values

```{r, eval=F, echo=T, class="large-code"}
quantile(x probs = 0.75)
```

-   Quartiles are 0.25,0.5,0.75 quantiles

```{r, eval=F, echo=T, class="large-code"}
quantile(x, probs = c(0.25, 0.5, 0.75))
```

-   Percentiles are 1-100% qunatiles

```{r, eval=F, echo=T, class="large-code"}
quantile(x, probs = seq(0.01,1,0.01))
```

-   Median is a **0.5 quantile** (2nd quartile)


## Normal distribution {auto-animate="true"}

::::: columns
::: {.column width="50%"}
Normal distribution is bell shaped, have equal mean ($\mu$), median, mode. "Width" depends on standard deviation ($\sigma$). **Continious (!)**
:::

::: {.column width="50%"}
$P(x) = \frac{1}{{\sigma \sqrt{2\pi} }}e^{{\frac{ -\left( {x - \mu } \right)^2 }{2\sigma ^2 }}}$,

parameters are mean ($\mu$) and sigma ($\sigma$)
:::
:::::

```{r}
Heights = rnorm(10000, mean = 175, sd = 10)
hist(Heights, breaks = 30, freq = T,
     main = 'Sample distribution of 10000 people\'s heights' )
```

## Normal distribution {auto-animate="true"}

### Parameters of normal distribution

```{r}
Heights_sd30 = rnorm(10000, mean = 100, sd = 30)
Heights_sd10 = rnorm(10000, mean = 120, sd = 10)

hist(Heights_sd30, breaks = 30, freq = T, 
     col = rgb(1, 0, 0, alpha = 0.5), ylim= c(0, 2000), 
     main='', xlab='x')
hist(Heights_sd10, breaks = 20, freq = T, 
     col = rgb(0, 0, 1, alpha = 0.5), add = TRUE)

legend("topright", legend = c("mean=100, sd = 30", "mean=120, sd = 10"), 
       fill = c(rgb(1, 0, 0, alpha = 0.5),
                rgb(0, 0, 1, alpha = 0.5)))
```

## Normal distribution {auto-animate="true"}

### Three sigma rule

```{r}
hist(Heights, breaks = 30, freq = T, ylim = c(0,1000))

abline(v=mean(Heights), col='green', lwd=5)
text(mean(Heights)+0.4*sd(Heights), 900, 'Mean', cex=1, )

abline(v=mean(Heights)+sd(Heights), col='blue', lwd=5)
abline(v=mean(Heights)-sd(Heights), col='blue', lwd=5)
text(mean(Heights), 500, '68%', cex=2, col = "blue")

text(mean(Heights)+1.4*sd(Heights), 900, ' + SD', cex=1)


abline(v=mean(Heights)+2*sd(Heights), col='orange', lwd=5)
abline(v=mean(Heights)-2*sd(Heights), col='orange', lwd=5)
text(mean(Heights)+1.5*sd(Heights), 500, '95%', 
     cex=1.5, col = "orange")

text(mean(Heights)+2.4*sd(Heights), 900, ' + 2 SD', cex=1)

abline(v=mean(Heights)+3*sd(Heights), col='red', lwd=5)
abline(v=mean(Heights)-3*sd(Heights), col='red', lwd=5)
text(mean(Heights)+2.5*sd(Heights), 500, '99.7%', 
     cex=1.3, col = "red")

text(mean(Heights)+3.4*sd(Heights), 900, ' + 3 SD', cex=1)

```

------------------------------------------------------------------------

### Three sigma rule

![](/images/memes/notesdistro.jpg){fig-align="center"}

## Uniform distribution

::::: columns
::: {.column width="70%"}
Simple distribution of equally possible values. **Continious (!).**
:::

::: {.column width="30%"}
$P(x) = \frac{1}{a-b}$

-   $a$ - starting point

-   $b$ - end point
:::
:::::

```{r, fig.width=10}
raindrops = runif(10000, min=0, max=10)

hist(raindrops, 
     breaks = 30, 
     freq = T,
     xlab = 'Coordinate',
     main = 'Sample distribution of a raindrop falling on\na line from 0 to 10 (10000 raindrops)')
# Probability of raindrop
```

## Bernoulli distribution

::::: columns
::: {.column width="50%"}
```{r, fig.width=5, fig.height=6}
coin_toss = rbinom(1000, size = 1, prob = 0.5)
barplot(table(coin_toss), cex.axis = 2, cex.names = 2)
```
:::

::: {.column width="50%"}
$P(1) = p$,\
$P(0) = 1-p$,

where $p$ - "success" probability. It is the only parameter of the distribution

Imagine, that $p$ is a probability of an head on a coin toss (1 - head, 0 - tails). Then The distribution will be like in the barplot on the left.

**Discrete (!)**
:::
:::::

## Binomial distribution {auto-animate="true"}

::::: columns
::: {.column width="70%"}
```{r}
coin_toss = rbinom(10000, size = 100, p=0.5)
hist(coin_toss, breaks = 30, freq = T,
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = 'Sample distribution of successes in 10000 experiments of 10x coin tosses')
```
:::

::: {.column width="30%"}
$P(x) = \binom{n}{k}p^k(1-p)^{n-k}$,

-   $n$ - number of trials *(fixed)*
-   $p$ - probability of success *(fixed)*
-   $k$ - observed successes
:::
:::::

Distribution is based on the **number of successes in a sequence of experiments**. **Discrete (!)**

If we flip a coin, the binomial distribution represents the number of successes after we flip the coin a certain number of times (e.g. 10).

The histogram above shows the distribution of 10000 experiments on trying to get an head coin for a certain number of times by flipping it 10 times

## Binomial distribution {auto-animate="true"}

### Parameters of binomial distribution

```{r }

coin_toss_100 = rbinom(10000, size = 100, p=0.5)
coin_toss_10 = rbinom(10000, size = 10, p=0.5)

coin_toss_left = rbinom(10000, size = 10, p=0.25)
coin_toss_right = rbinom(10000, size = 10, p=0.75)

par(mfrow=c(1,2))
hist(coin_toss_left, breaks = 10, freq = T, 
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = "", 
     xlim=c(0, 12), 
     ylim = c(0,3500),
     col = rgb(0, 0, 1, alpha = 0.5))

hist(coin_toss_right, breaks = 10, freq = T, 
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = "", add=T, 
     col = rgb(1, 0, 0, alpha = 0.5))


legend("topright", legend = c("p=0.75",
                              "p=0.25"), 
       fill = c(rgb(1, 0, 0, alpha = 0.5),
                rgb(0, 0, 1, alpha = 0.5)))

hist(coin_toss_100, breaks = 20, freq = T, 
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = "", 
     xlim=c(0, 100), 
     ylim = c(0,2500),
     col = rgb(1, 0, 0, alpha = 0.5))

hist(coin_toss_10, breaks = 10, freq = T, 
     add=T, 
     col = rgb(0, 0, 1, alpha = 0.5))

legend("topright", legend = c("n=10",
                              "n=100"), 
       fill = c(rgb(0, 0, 1, alpha = 0.5),
                rgb(1, 0, 0, alpha = 0.5)))
```

## Poisson distribution {auto-animate="true"}

*Binomial*, but $n \rightarrow \infty$,, therefore we don't utilize number of trials ($n$).\
Now we use "**time interval**" and **expected number of successes** ($\lambda$) during this interval. **Still Discrete (!)**

::::: columns
::: {.column width="70%"}
```{r}
pois_rand = rpois(10000, 10)

hist(pois_rand, breaks = 30, freq = T, 
     xlab = 'Number of successes',
     main = 'Sample distribution of the number of \nlightning strikes per hour (10 expected, recorded 10000 h)',
     xlim = c(0, 30))
```
:::

::: {.column width="30%"}
$P(x) = \dfrac{\lambda^k}{k!}e^{-\lambda}$, where

-   $\lambda$ - mean or expected number of successes during the interval (fixed)
-   $k$ - "observed" successes
:::
:::::

<!-- ------------------------------------------------------------------------ -->

## Poisson distribution {auto-animate="true"}

### Parameters of poisson distribution

```{r}
pois_rand = rpois(10000, 10)
pois_rand_2 = rpois(10000, 60)
pois_rand_3 = rpois(10000, 3)

hist(pois_rand, breaks = 30, freq = T, 
     xlab = 'Number of successes',
     # ylab = "Number of experiments", 
     ylim = c(0, 2800),
     xlim = c(0, 100),
     main = "",
     col = rgb(0, 0, 1, alpha = 0.5),
     )

hist(pois_rand_2, breaks = 30, freq = T,
     col = rgb(0, 1, 0, alpha = 0.5),
     add = T)

hist(pois_rand_3, breaks = 10, freq = T, 
     col = rgb(1, 0, 0, alpha = 0.5),
     add = T)


legend("topright", legend = c("lambda=3", 
                              "lambda=10",
                              "lambda=60"), 
       fill = c(rgb(1, 0, 0, alpha = 0.5),
                rgb(0, 0, 1, alpha = 0.5),
                rgb(0, 1, 0, alpha = 0.5)))
```

## Functions (for normal distribution)

```{css}
.large-code {
  font-size: 1.25em; /* Increase as needed, e.g., 1.5em or 20px */
}
```

1.  Random values from distribution

```{r, echo = T, class="large-code"}
rnorm(n=5, mean=100, sd=10)
```

2.  Density (the height of histogram) at the point

```{r, echo = T, class="large-code"}
dnorm(x=100, mean=100, sd=10)
```

3.  Cumulative probability at the point: prob-ty to obtain value less than specified

```{r, echo = T, class="large-code"}
pnorm(q = 80, mean=100, sd=10) # prob to obtain any value less than 80
pnorm(q = c(70,80,90,110,120,130), mean=100, sd=10)
```

4.  Value corresponding to specified cumulative probability

```{r, echo = T, class="large-code"}
qnorm(p = c(0.003, 0.05, 0.16, 0.84, 0.95, 0.997), mean=100, sd=10)
```

## Functions (for other distributions)

**Uniform**

```{r, eval=F, echo=TRUE,class="large-code"}
runif(n, min, max)
dunif(n, min, max)
punif(n, min, max)
qunif(n, min, max)
```

**Binomial**

```{r, eval=F, echo=TRUE,class="large-code"}
rbinom(n, size, prob)
dbinom(n, size, prob)
pbinom(n, size, prob)
qbinom(n, size, prob)
```

**Poisson**

```{r, eval=F, echo=TRUE,class="large-code"}
rpois(n, lambda)
dpois(n, lambda)
ppois(n, lambda)
qpois(n, lambda)
```

# Basics of testing

```{css}
.faded-text {
  opacity: 0.4;
}
```

## Testing plan

1\) What we want to test? **Hypotheses**

::: faded-text
2\) Choose significance level

3\) Choose the test

4\) Conduct the test

5\) Conclusion
:::

## Hypotheses

In statistical testing, we will operate with **hypotheses.** *There are two basic hypotheses:*

-   **Null Hypothesis (**$H_0$**)** - There is **no effect** in the population.
-   **Alternative Hypothesis (**$H_A$**) -** There is **a effect** in the population.

> Usually we are interested in having an effect, as it means some discovery, and **rejecting null hypothesis**

## Testing plan

::: faded-text
1\) What we want to test? **Hypotheses**
:::

2\) Choose significance level

::: faded-text
3\) Choose the test

4\) Conduct the test

5\) Conclusion
:::

------------------------------------------------------------------------

::::: columns
::: {.column width="50%"}
### Type I error

Error of rejecting $H_0$ when $H_0$ is true
:::

::: {.column width="50%"}
### Type II error

Error of accepting $H_0$ when $H_0$ is false
:::
:::::

<br>

|                     | Reality: **True** | Reality: **False** |
|---------------------|:------------------|:-------------------|
| Measured: **True**  | Correct ð        | Type 1 Error       |
| Measured: **False** | Type 2 Error      | Correct ð         |

------------------------------------------------------------------------

Probability of Type I error ($\alpha$) (= **p-value** after testing)

Probability of Type II error ($\beta$)

![](../images/typeserror.png){fig-align="center" width="600"}

## Significance level {auto-animate="true"}

Before testing we must define value $\alpha$ named **significance level**

**Significance level (**$\alpha$) - **predetermined threshold of rejecting** $H_0$.

> For example we set $\alpha = 0.05$ and got **p-value** \< $\alpha$:
>
> We reject the $H_0$ at the $\alpha = 0.05$ significance level, indicating that there is sufficient evidence to conclude that the observed result is unlikely to have occurred under the null hypothesis.

Thus we [**fix the Type 1 Error**]{.underline} , but [**Type 2 Error still can be large!**]{.underline}

The value $1-\beta$ called the [**power of the test**]{.underline}. *The smaller is the probability of type II error, the more powerful is the test.*

## Significance level {auto-animate="true"}

### Examples

-   **Medical/Pharmaceutical**: $\alpha = 0.01$ is common to avoid false positives.
-   **Psychology/Social**: Typically $\alpha = 0.05$.
-   **Economics/Business**: Sometimes $\alpha = 0.1$ is acceptable

## Testing plan

::: faded-text
1\) What we want to test? **Hypotheses**

2\) Choose significance level
:::

3\) Choose the test

4\) Conduct the test

::: faded-text
5\) Conclusion
:::

## Test result

The tests result in [**test statistic**]{.underline} and the [**p-value**]{.underline} .

-   Each test have its formula to calculate called [**test statistic**]{.underline}. This value has its own distribution (*e.g. t-distribution for the t-statistic from t-test*).

-   [**p-value**]{.underline} - the probability that, given a **true null hypothesis**, your observations will result **in such or more extreme value of test statistic**

> The smaller the p-value, the less likely we are to get our result under the null hypothesis, which states that there is no effect.

## "Pipeline"

![](/images/pvalpipeline.png){fig-align="center"}

## Parameters of test parameters' distributions

### [Degrees of freedom!]{.underline}

-   Often calculated based on [number of observations]{.underline}, groups and coefficients.

-   [Determines the shape of distribution!]{.underline}

> **-\> Amount of data affects testing results!**

#### Other parameters...

-   Mean, SD

-   Rate, shape

## t-distribution for different DFs

![](/images/dfsT.png){fig-align="center" width="600"}

```{r, warning=FALSE}
# t1 = rt(100000, 3)
# t2 = rt(100000, 3)
# t5 = rt(100000, 5)
# t10 = rt(100000, 10)
# t30 = rt(100000, 30)
# t100 = rt(100000, 100)
# 
# dft = data.frame(t1,t2,t5,t10,t30,t100) %>%  
#   pivot_longer(values_to = 'value', 
#                cols = 1:5, names_to = 't') %>% 
#   mutate(t = as.factor(t))
# 
# dft %>%  ggplot(aes(col=t, 
#                     x=value, 
#                     y=..density..)) + 
#   # geom_histogram(alpha=0.4, bins = 100) +
#   geom_density() +
#   xlim(c(-5,5))

```

## *p-value* calculation

![](../images/memes/p-value.jpg){fig-align="center"}

------------------------------------------------------------------------

### *p-value* $\geq \alpha$

Since the ***p-value*** greater than chosen significance level ($\alpha$) we **fail** to reject $H_0$

### *p-value* $< \alpha$

Since the ***p-value*** is less than chosen significance level ($\alpha$), we reject $H_0$ and conclude that there is sufficient statistical evidence to support $H_A$

------------------------------------------------------------------------

![](/images/memes/p_values_2x.png){fig-align="center" width="498"}

## One-sided hypothesis

$H_A: X_0 < X_1$

```{r}
plot(x=seq(-5,5,0.01), y=dt(seq(-5,5,0.01), df=19), 'l', lwd=2,
     main = 'T-distribution df=19',
     xlab='t', ylab='density')
abline(v=qt(0.95, df=19), col='red', lwd=2) 
text(x =3, y = 0.3, col='red',
     labels = paste('alpha = 0.05\nt =', qt(0.95, df=19) %>%round(2)), cex=1.3)

points(x = c(1.5, 3.1,-3.95), y = rep(0.03, 3), pch=c(13,19,13), cex=2)
text(x = c(1.1, 3.1,-3.95), y = rep(0.07, 3), 
     labels = c('t = 1.5', 't = 3.1', 't = -3.95'), cex=1.5)
```

```{r}
# plot(x=seq(0, 10,0.01), y=dchisq(seq(0,10,0.01), df=2), 'l', lwd=2,
#      main = 'Chi-squared distribution df=2', xlab='Chisq', ylab='density')
# abline(v=qchisq(0.95, df=2), col='red', lwd=2)
# text(x =7, y = 0.3, col='red',
#      labels = 'alpha = 0.05', cex=1)
# 
# points(x = c(2.1, 7.34), y = rep(0.03, 2), pch=c(13,19), cex=2)
# text(x = c(2.1, 7.34), y = rep(0.07, 3), 
#      labels = c('chisq. = 2.1', "chisq. = 7.34"), cex=1.2)
```

## Two-sided hypothesis

$H_A: X_0 \neq X_1$

```{r}
plot(x=seq(-5,5,0.01), y=dt(seq(-5,5,0.01), df=19), 'l', lwd=2,
     main = 'T-distribution df=19',
     xlab='t', ylab='density')
abline(v=qt(0.975, df=19), col='red', lwd=2) 
abline(v=qt(0.025, df=19), col='red', lwd=2)
text(x =c(-3,3), y = 0.3, col='red',
     labels = c(paste('alpha / 2\nt =', qt(0.025, df=19) %>% round(2)),
                paste('alpha / 2\nt =', qt(0.975, df=19) %>% round(2))), cex=1.3)


points(x = c(1.5, 3.1,-3.95), y = rep(0.03, 3), pch=c(13,19,19), cex=2)
text(x = c(1.2, 3.1,-3.95), y = rep(0.07, 3), 
     labels = c('t = 1.5', 't = 3.1', 't = -3.95'), cex=1.5)
```

## How to calculate critical value of parameter?

*Example for T-distribution*

### One-sided hypotheses

::: large-code
```{r, echo=T}
qt(0.95, df=19)
qt(0.05, df=19)
```
:::

### Two-sided hypothesis

::: large-code
```{r, echo=T}
qt(0.975, df=19)
qt(0.025, df=19)
```
:::

------------------------------------------------------------------------

![](../images/t-table.png){fig-align="center" width="700"}

## Another pitfalls

1.  **Small sample size** or **Unbalanced groups**
    -   Power of tests will be reduced
2.  **Changing significance level**
    -   $\alpha$ is defined **BEFORE** testing
3.  **Misinterpreting *p-values***
    -   It doesn't reject or accept $H_0$.

    -   **YOU reject or accept** $H_0$
4.  [**Multiple comparisons problem**]{.underline}

## Multiple comparisons {auto-animate="true"}

-   $1 - 0.05$ - probability of correctly rejected $H_0$

-   $(1-0.05)^n$ - probability of correctly rejected $\space H_0$ in $n$ tests

-   $1-(1-0.05)^n$ - probability of [**incorrectly rejected at least 1**]{.underline} $H_0$ in $n$ tests

$$
1 - (1 - 0.05) ^ {2} = 0.0975\\
1 - (1 - 0.05) ^ {5} = 0.2262191\\
1 - (1 - 0.05) ^ {20} = 0.6415141\\
1 - (1 - 0.05) ^ {100} = 0.9940795\\
$$

## Multiple comparisons {auto-animate="true"}

### Corrections!

Controlling the **FWER** - probability of making at least one Type I error.

-   Bonferroni

-   Holm-Bonferroni

-   Å idÃ¡k

Controlling the **FDR** - expected proportion of false positives among all significant tests.

-   Benjamini-Hochberg

-   Benjamini-Yekutieli

# Let's go
