---
title: "Distributions and statistics"
format: 
  revealjs:
      center: true
      controls: true
      # controlsLayout: 'bottom-left'
      slide-number: true
      show-slide-number: all
      fontsize: 30px
      style: style.css
      progress: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.width = 8, fig.height = 4, dpi = 300)
library(dplyr)
library(tidyr)
library(ggplot2)

theme_custom <- theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 20),
    plot.title = element_text(size = 20, hjust = 0.5),
    plot.subtitle = element_text(size = 15, hjust = 0.5),
    legend.title = element_text(size = 15),
    legend.text = element_text(size = 15),
    panel.background = element_rect(fill = "white"), 
    panel.grid = element_blank()
  )

theme_set(theme_custom)
```

![](../images/memes/belike.jpg){fig-align="center" width="1000"}

------------------------------------------------------------------------

## Types of Data

-   **Numerical data**: Continuous (e.g., height) or discrete (e.g., number of leaves).
-   **Categorical data**: Nominal (e.g., species, gender) or ordinal (e.g., rankings).

![](/images/DataTypes.png){fig-align="center" width="1800"}

## Sampling {auto-animate="true"}

Usually we **cannot take the whole population** for a study.

> For example, it is physically impossible to do a study using blood samples from all the people in the world. Sometimes the population is small, but it is extremely expensive to collect the material.

Therefore, researchers use samples - **random and representative** groups taken from the general population that can be analysed to **provide information about the whole population.**

### This is where statistics come to the rescue.

## Sampling {auto-animate="true"}

### Are these samples are representative?

1.  20 ITMO Biology students to study average marks of students in ITMO
2.  100 ITMO first-year students to study students workload in their first ever exam
3.  Email survey on student parents' opinion of ITMO
4.  The double-blind, randomized, placebo-controlled trial in 1990

------------------------------------------------------------------------

1.  20 ITMO Biology students to study average marks of students in ITMO
    -   **Representive only for Biology students**
2.  100 ITMO first-year students to study students workload
    -   **Represetive only for ITMO. Students with high workloads are less likely to participate**
3.  Email survey on student parents' opinion of ITMO
    -   **Only people whose email addresses we have**
4.  The double-blind, randomized, placebo-controlled trial in 1990
    -   **May be too old**

------------------------------------------------------------------------

### [Results of the student survey, 2024 (449 people)](https://fitp.itmo.ru/p/abit/804)

::::: columns
::: {.column width="45%"}
![](/images/memes/itmosurvey.png){fig-align="center" width="1000"}
:::

::: {.column width="55%"}
![](/images/memes/itmopie.png){fig-align="center"} ![](/images/memes/itmolegend.png){fig-align="center"}
:::
:::::

------------------------------------------------------------------------

::::: columns
::: {.column width="50%"}
![](/images/memes/roulette.jpg){fig-align="center" width="500"}
:::

::: {.column width="50%"}
![](/images/memes/selection_bias.png){fig-align="center" width="500"}
:::
:::::

## Sampling {auto-animate="true"}

![](../images/sampling_types.jpg){fig-align="center" width="800"}

## Sampling {auto-animate="true"}

::::: columns
::: {.column width="50%"}
```{r, fig.height=5, fig.width=6}
normgenpop = rnorm(100000) %>%  as.data.frame(nm = 'x')
normgenpop %>% ggplot(aes(x=x)) +
geom_density(col='black', fill="lightblue") +
labs(title='General population (100000 objects)')
    # theme(title = element_text(size=20))

# normgenpop = rnorm(100000) 
# hist(normgenpop, br=50, xlab='X', main = 'General population')
```
:::

::: {.column .fragment .fade-in width="50%"}
```{r, fig.height=5, fig.width=6}
ggplot() + 
  geom_density(data=sample_n(normgenpop, 10), fill='lightblue1',
                 aes(x=x), alpha=0.4) +
  geom_density(data=sample_n(normgenpop, 10),fill='green', 
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 10), fill='coral',
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 10), fill = 'violet',
                 aes(x=x), alpha=0.4) +
  # theme_classic() +
  labs(title='4 samples of 10 objects')
  # theme(title = element_text(size=20))
```
:::
:::::

------------------------------------------------------------------------

### The larger the sample, the more similar it is to the general population

```{r,}
ggplot() + 
  geom_density(data=sample_n(normgenpop, 1000), fill='lightblue1',
                 aes(x=x), alpha=0.4) +
  geom_density(data=sample_n(normgenpop, 1000),fill='green', 
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 1000), fill='coral',
                 aes(x=x), alpha=0.4) + 
  geom_density(data=sample_n(normgenpop, 1000), fill = 'violet',
                 aes(x=x), alpha=0.4) +
  # theme_classic() + 
  labs(title='4 samples of 1000 objects') 
  # theme(title = element_text(size=20))
```

# Distributions

## Distributions

::::: columns
::: {.column width="50%"}
### Discrete

Countable number of values

-   Bernoulli
-   Poisson
-   Binomial
:::

::: {.column width="50%"}
### Continious

$\infty$ number of values

-   Normal
-   Uniform
-   Chi-squared
:::
:::::

\[**Discrete** $\rightarrow$ Continuous\] if \[number of values $\rightarrow \infty$\]

## Probability of discrete variable

::::: columns
::: {.column width="40%"}
```{r message=FALSE}
#| fig-height: 6
#| fig-width: 6

sample(c(-1,0,1), 10000, replace = T, prob = c(0.3, 0.4, 0.3)) |> 
  table() |>  prop.table() |> 
  barplot(ylim = c(0,0.6), ylab='PROBABILITY')
```
:::

::: {.column width="60%"}
Increasing the discreteness

```{r}

#| fig-height: 6
#| fig-width: 6

sample(seq(-1,1,0.25), 10000, replace = T) |> 
  table() |>  prop.table() |> 
  barplot(ylim = c(0,0.6), ylab='PROBABILITY')

```
:::
:::::

The probability of each value $\rightarrow 0$

## Probability of continious variable {auto-animate="true"}

$\infty$ values and $\infty$ discreteness

### Probability of each value = 0 ???

## Probability of continious variable {auto-animate="true"}

$\infty$ values and $\infty$ discreteness

### NO!

but Probability of each particular value $\rightarrow 0$

## Probability density

### The way how continious distributions are described

*Area under the plot should be = 1*

```{r, warning=FALSE}
rnorm(mean=0, sd = 0.01, n = 10000) |> as.data.frame(nm='value') |> 
ggplot() + 
  geom_histogram(aes(x = value, y = after_stat(density)), 
                 # binwidth=0.2, 
                 col='black', fill='lightblue') +
  # xlim(c(-1,11)) + 
  ylab('DENSITY')
```

*Density value in some point can be way greater than 1!*

## Normal distribution {auto-animate="true"}

::::: columns
::: {.column width="55%"}
Normal distribution is bell shaped, have equal mean ($\mu$), median, mode.\
"Width" depends on standard deviation ($\sigma$). **Continious (!)**
:::

::: {.column width="45%"}
$P(x) = \frac{1}{{\sigma \sqrt{2\pi} }}e^{{\frac{ -\left( {x - \mu } \right)^2 }{2\sigma ^2 }}}$,

parameters: mean ($\mu$) and sigma ($\sigma$) [(it **does not** exist in nature)]{style="color:red;"}
:::
:::::

```{r}
Heights = rnorm(10000, mean = 175, sd = 10)
hist(Heights, breaks = 30, freq = T,
     # main = 'Sample distribution of 10000 people\'s heights' 
     )
```

## Normal distribution {auto-animate="true"}

### Parameters of normal distribution

```{r}
Heights_sd30 = rnorm(10000, mean = 100, sd = 30)
Heights_sd10 = rnorm(10000, mean = 120, sd = 10)

hist(Heights_sd30, breaks = 30, freq = T, 
     col = rgb(1, 0, 0, alpha = 0.5), ylim= c(0, 2000), 
     main='', xlab='x')
hist(Heights_sd10, breaks = 20, freq = T, 
     col = rgb(0, 0, 1, alpha = 0.5), add = TRUE)

legend("topright", legend = c("mean=100, sd = 30", "mean=120, sd = 10"), 
       fill = c(rgb(1, 0, 0, alpha = 0.5),
                rgb(0, 0, 1, alpha = 0.5)))
```

## Normal distribution {auto-animate="true"}

### Three sigma rule

```{r}
hist(Heights, breaks = 30, freq = T, ylim = c(0,1000))

abline(v=mean(Heights), col='green', lwd=5)
text(mean(Heights)+0.4*sd(Heights), 900, 'Mean', cex=1, )

abline(v=mean(Heights)+sd(Heights), col='blue', lwd=5)
abline(v=mean(Heights)-sd(Heights), col='blue', lwd=5)
text(mean(Heights), 500, '68%', cex=2, col = "blue")

text(mean(Heights)+1.4*sd(Heights), 900, ' + SD', cex=1)


abline(v=mean(Heights)+2*sd(Heights), col='orange', lwd=5)
abline(v=mean(Heights)-2*sd(Heights), col='orange', lwd=5)
text(mean(Heights)+1.5*sd(Heights), 500, '95%', 
     cex=1.5, col = "orange")

text(mean(Heights)+2.4*sd(Heights), 900, ' + 2 SD', cex=1)

abline(v=mean(Heights)+3*sd(Heights), col='red', lwd=5)
abline(v=mean(Heights)-3*sd(Heights), col='red', lwd=5)
text(mean(Heights)+2.5*sd(Heights), 500, '99.7%', 
     cex=1.3, col = "red")

text(mean(Heights)+3.4*sd(Heights), 900, ' + 3 SD', cex=1)

```

------------------------------------------------------------------------

### Three sigma rule

![](/images/memes/notesdistro.jpg){fig-align="center"}

## Uniform distribution

::::: columns
::: {.column width="70%"}
Simple distribution of equally possible values. **Continious (!).**
:::

::: {.column width="30%"}
$P(x) = \frac{1}{a-b}$

-   $a$ - starting point

-   $b$ - end point
:::
:::::

```{r, fig.width=10}
raindrops = runif(10000, min=0, max=10)

hist(raindrops, 
     breaks = 30, 
     freq = T,
     xlab = 'Coordinate',
     main = 'Sample distribution of a raindrop falling on\na line from 0 to 10 (10000 raindrops)')
# Probability of raindrop
```

<!-- ## Bernoulli distribution -->

<!-- ::::: columns -->

<!-- ::: {.column width="50%"} -->

<!-- ```{r, fig.width=5, fig.height=6} -->

<!-- coin_toss = rbinom(1000, size = 1, prob = 0.5) -->

<!-- barplot(table(coin_toss), cex.axis = 2, cex.names = 2) -->

<!-- ``` -->

<!-- ::: -->

<!-- ::: {.column width="50%"} -->

<!-- $P(1) = p$,\ -->

<!-- $P(0) = 1-p$, -->

<!-- where $p$ - "success" probability. It is the only parameter of the distribution -->

<!-- Imagine, that $p$ is a probability of an head on a coin toss (1 - head, 0 - tails). Then The distribution will be like in the barplot on the left. -->

<!-- **Discrete (!)** -->

<!-- ::: -->

<!-- ::::: -->

## Exponential distribution

::::: columns
::: {.column width="60%"}
```{r, fig.width=7, fig.height=4, warning=F}

ggplot() +
  geom_density(data=rexp(rate = 1, n=100000) %>% data.frame(value = .),
               aes(x=value,  col='1'),
               # col='red'
               ) +
  geom_density(data=rexp(rate = 5, n=100000) %>% data.frame(value = .), 
               aes(x=value, col='5'),
               # col='blue'
               ) +
  xlim(c(0,3)) + 
  scale_color_manual(values=c('1'='red', '5'='blue'), 
                     name='Lambda')
```
:::

::: {.column width="40%"}
$P(X) = \lambda e^{-\lambda x}$

How fast something appears (*mortality*)

**Continious**
:::
:::::

## Binomial distribution {auto-animate="true"}

::::: columns
::: {.column width="70%"}
```{r}
coin_toss = rbinom(10000, size = 100, p=0.5)
hist(coin_toss, breaks = 30, freq = T,
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = 'Sample distribution of successes in 10000 experiments of 10x coin tosses')
```
:::

::: {.column width="30%"}
$P(x) = \binom{n}{k}p^k(1-p)^{n-k}$,

-   $n$ - number of trials *(fixed)*
-   $p$ - probability of success *(fixed)*
-   $k$ - observed successes
:::
:::::

Distribution is based on the **number of successes in a sequence of experiments**. **Discrete (!)**

If we flip a coin, the binomial distribution represents the number of successes after we flip the coin a certain number of times (e.g. 10).

The histogram above shows the distribution of 10000 experiments on trying to get an head coin $k$ times by flipping it 10 times

## Binomial distribution {auto-animate="true"}

### Parameters of binomial distribution

```{r }

coin_toss_100 = rbinom(10000, size = 100, p=0.5)
coin_toss_10 = rbinom(10000, size = 10, p=0.5)

coin_toss_left = rbinom(10000, size = 10, p=0.25)
coin_toss_right = rbinom(10000, size = 10, p=0.75)

par(mfrow=c(1,2))
hist(coin_toss_left, breaks = 10, freq = T, 
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = "", 
     xlim=c(0, 12), 
     ylim = c(0,3500),
     col = rgb(0, 0, 1, alpha = 0.5))

hist(coin_toss_right, breaks = 10, freq = T, 
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = "", add=T, 
     col = rgb(1, 0, 0, alpha = 0.5))


legend("topright", legend = c("p=0.75",
                              "p=0.25"), 
       fill = c(rgb(1, 0, 0, alpha = 0.5),
                rgb(0, 0, 1, alpha = 0.5)))

hist(coin_toss_100, breaks = 20, freq = T, 
     xlab = 'Number of heads received (k)', 
     ylab = "Number of experiments",
     main = "", 
     xlim=c(0, 100), 
     ylim = c(0,2500),
     col = rgb(1, 0, 0, alpha = 0.5))

hist(coin_toss_10, breaks = 10, freq = T, 
     add=T, 
     col = rgb(0, 0, 1, alpha = 0.5))

legend("topright", legend = c("n=10",
                              "n=100"), 
       fill = c(rgb(0, 0, 1, alpha = 0.5),
                rgb(1, 0, 0, alpha = 0.5)))
```

## Poisson distribution {auto-animate="true"}

*Binomial*, but $n \rightarrow \infty$,, therefore we don't utilize number of trials ($n$).\
Now we use "**time interval**" and **expected number of successes** ($\lambda$) during this interval.\
**Still Discrete (!)**

::::: columns
::: {.column width="70%"}
```{r}
pois_rand = rpois(10000, 10)

hist(pois_rand, breaks = 30, freq = T, 
     xlab = 'Number of successes',
     main = 'Sample distribution of the number of \nlightning strikes per hour (10 expected, recorded 10000 h)',
     xlim = c(0, 30))
```
:::

::: {.column width="30%"}
$P(x) = \dfrac{\lambda^k}{k!}e^{-\lambda}$, where

-   $\lambda$ - mean or expected number of successes during the interval (fixed)
-   $k$ - "observed" successes
:::
:::::

<!-- ------------------------------------------------------------------------ -->

## Poisson distribution {auto-animate="true"}

### Parameters of poisson distribution

```{r}
pois_rand = rpois(10000, 10)
pois_rand_2 = rpois(10000, 60)
pois_rand_3 = rpois(10000, 3)

hist(pois_rand, breaks = 30, freq = T, 
     xlab = 'Number of successes',
     # ylab = "Number of experiments", 
     ylim = c(0, 2800),
     xlim = c(0, 100),
     main = "",
     col = rgb(0, 0, 1, alpha = 0.5),
     )

hist(pois_rand_2, breaks = 30, freq = T,
     col = rgb(0, 1, 0, alpha = 0.5),
     add = T)

hist(pois_rand_3, breaks = 10, freq = T, 
     col = rgb(1, 0, 0, alpha = 0.5),
     add = T)


legend("topright", legend = c("lambda=3", 
                              "lambda=10",
                              "lambda=60"), 
       fill = c(rgb(1, 0, 0, alpha = 0.5),
                rgb(0, 0, 1, alpha = 0.5),
                rgb(0, 1, 0, alpha = 0.5)))
```

## Descriptive statistics {auto-animate="true"}

### *Central tendency*

-   **Sample mean** - $\bar{X} \ or\ \hat\mu = \frac{x_1 + x_2 + ... x_N}{N}$
-   **Population mean -** $E(X) = x_1*p_1 + x_2*p_2 + ... x_N * p_N = \sum^N_{i=1} x_i*p_i$

```{r, eval=F, echo=T, class="large-code"}
mean(x)
sum(x * p) # x and p are vectors
```

-   **Median** - the $N/2$-th element in list of sorted values $\{x_1, x_2, ..., \pmb{x_{N/2}},...,x_{N-1}, x_N\}$, where for any $i$: $x_{i-1} < x_i$

```{r, eval=F, echo=T, class="large-code"}
median(x)
```

-   **Mode** - The most frequent value *one of the methods: only for discrete data:*

```{r, eval=F, echo=T, class="large-code"}
x %>% table() %>% sort(decreasing = T) %>% head(1) %>% names() %>% as.numeric()
```

```{r, eval=T, echo=F, class="large-code"}
mode_fun = function(x) x %>% table() %>% sort(decreasing = T) %>% head(1) %>% names() %>% as.numeric()
```

***They are equal for a normal distribution***

## Descriptive statistics {auto-animate="true"}

### Bimodal data

```{r, eval=T, echo=F,  fig.width = 8, fig.height = 5}
twomodes = c(rbinom(10000, 31, 0.4), rbinom(10000, 31, 0.7))

hist(twomodes, breaks=30, 
     main=NULL,
     xlab='X')
abline(v = 30*0.4, lwd=3, col='red')
abline(v = 31*0.7, lwd=3, col='red')
abline(v = mean(twomodes), lwd=3, col='blue')
abline(v = median(twomodes), lwd=3, col='green3')

legend("topright",
       col = c('red', 'blue', 'green3'), 
       legend = c('modes', 'mean', "median"), 
        lwd = 3, bty = "n")

```

## Descriptive statistics {auto-animate="true"}

### Exponential distribution

```{r, eval=T, echo=F,  fig.width = 7, fig.height = 4}
expsmall = rexp(10000, 10)

hist(expsmall, breaks=30, 
     main=NULL,
     xlab='X')
abline(v = mode_fun(expsmall), lwd=3, col='red')
abline(v = mean(expsmall), lwd=3, col='blue')
abline(v = median(expsmall), lwd=3, col='green3')

legend("topright",
       col = c('red', 'blue', 'green3'), 
       legend = c('mode', 'mean', "median"), 
        lwd = 3, bty = "n")

```

## Descriptive statistics {auto-animate="true"}

### Outliers

**Mean is more sensitive to outliers**

```{r, eval=T, echo=F,  fig.width = 8, fig.height = 4}
normoutlier = c(rnorm(100), 50, 50, 50)

hist(normoutlier, breaks=60, 
     main=NULL,
     xlab='X')
# abline(v = 0, lwd=3, col='red')
abline(v = mean(normoutlier), lwd=3, col='blue')
abline(v = median(normoutlier), lwd=3, col='green3')

# text(x=7, y=10, label='mean')

legend("topright",
       col = c(
         # 'red',
         'blue', 'green3'), 
       legend = c(
         # 'mode',
         'mean', "median"), 
        lwd = 3, bty = "n")

```

## By the way, what to do about outliers?

**Nothing**

-   If they still contain some information.Â 

**Delete**

-   If there are few of them.
-   This is an obvious error and [**you are sure**]{.underline} they do not contain valuable information

## Descriptive statistics {auto-animate="true"}

### Variance and Standard deviation

$Var(X) = S^2 = \dfrac{((x_1 - \bar{X})^2 + (x_2 - \bar{X})^2 + ... + (x_N - \bar{X})^2)}{N-1} = \dfrac{\sum{(x_i - \bar{X})^2}}{N-1}$

```{r, eval=F, echo=T, class="large-code"}
var(x)
```

$SD(X) = S = \sqrt{\dfrac{\sum{(x_i - \bar{X})^2}}{N-1}}$

```{r, eval=F, echo=T, class="large-code"}
sd(x)
```

> For the **population** variance in the denominator of the formula is $N$, not $N-1$

## Descriptive statistics {auto-animate="true"}

### Quantiles

**Quantiles** - values less than which $X$% of the data in the distribution is found\
*For example*, 0.75 quntile is the value that is greater of 75% of values in sample, but lower than another 25% of sample values

```{r, eval=F, echo=T, class="large-code"}
quantile(x, probs = 0.75)
```

-   Quartiles are 0.25,0.5,0.75 quantiles

```{r, eval=F, echo=T, class="large-code"}
quantile(x, probs = c(0.25, 0.5, 0.75))
```

-   Percentiles are 1-100% qunatiles

```{r, eval=F, echo=T, class="large-code"}
quantile(x, probs = seq(0.01, 1, 0.01))
```

-   Median is a **0.5 quantile** (2nd quartile)

## Functions (for normal distribution) {auto-animate="true"}

```{css}
.large-code {
  font-size: 1.25em; /* Increase as needed, e.g., 1.5em or 20px */
}
```

1.  **Random values from distribution**

```{r, echo = T, class="large-code"}
rnorm(n=5, mean=100, sd=10)
```

2.  **Density (the height of histogram) at the point**

```{r, echo = T, class="large-code"}
dnorm(x=100, mean=100, sd=10)
```

## Functions (for normal distribution) {auto-animate="true"}

3.  **Cumulative probability at the point: prob-ty to obtain value less than specified**

```{r, echo = T, class="large-code"}
pnorm(q = 80, mean=100, sd=10) # prob to obtain any value less than 80
pnorm(q = c(70,80,90,110,120,130), mean=100, sd=10)
```

4.  **Value corresponding to specified cumulative probability**

```{r, echo = T, class="large-code"}
qnorm(p = c(0.003, 0.05, 0.16, 0.84, 0.95, 0.997), mean=100, sd=10)
```

## Functions (for other distributions)

**Uniform**

```{r, eval=F, echo=TRUE,class="large-code"}
runif(n, min, max)
dunif(n, min, max)
punif(n, min, max)
qunif(n, min, max)
```

**Binomial**

```{r, eval=F, echo=TRUE,class="large-code"}
rbinom(n, size, prob)
dbinom(n, size, prob)
pbinom(n, size, prob)
qbinom(n, size, prob)
```

**Poisson**

```{r, eval=F, echo=TRUE,class="large-code"}
rpois(n, lambda)
dpois(n, lambda)
ppois(n, lambda)
qpois(n, lambda)
```

# Central limit theorem

## Standard error (SE) {auto-animate="true"}

*"SD of means of samples"*

::::: columns
::: {.column width="30%"}
$SE = \dfrac{SD}{\sqrt n}$
:::

::: {.column width="70%"}
1.  Several samples
2.  Calculate mean in each sample
3.  Build the means distribution
4.  Calculate resulting distribution's SD
:::
:::::

## Standard error (SE) {auto-animate="true"}

*"SD of means of samples"*

::::: columns
::: {.column width="15%"}
$SE = \dfrac{SD}{\sqrt n}$
:::

::: {.column width="85%"}
```{r}
sample_size = 100 
n_samples = 1000

df_trial <- data.frame(
  sample_ID = rep(1:n_samples, each=sample_size),
  value = runif(sample_size*n_samples, 0, 10)
  # value = rnorm(sample_size*n_samples, 0, 2)
  # value = rexp(sample_size*n_samples, 1)
  )

df_trial |> group_by(sample_ID) |> 
  summarize(m = mean(value)) |> 
  ggplot() + geom_histogram(aes(x=m), bins=30, col='black', fill='skyblue')
```
:::
:::::

## Magic of Central Limit Theorem {auto-animate="true"}

This means' distribution was built from **uniform distribution** samples! $a=0, b=10 \Rightarrow \hat\mu=5$

```{r}
sample_size = 1000
n_samples = 1000

df_trial <- data.frame(
  sample_ID = rep(1:n_samples, each=sample_size),
  value = runif(sample_size*n_samples, 0, 10)
  # value = rnorm(sample_size*n_samples, 0, 2)
  # value = rexp(sample_size*n_samples, 1)
  )

df_trial |> group_by(sample_ID) |> 
  summarize(m = mean(value)) |> 
  ggplot() + geom_histogram(aes(x=m), bins=30, col='black', fill='skyblue')
```

**Mean is also = 5!**

## Magic of Central Limit Theorem {auto-animate="true"}

**Let's do it with exponential distribution samples** ($\lambda=2$)

```{r}
sample_size = 1000
n_samples = 1000

df_trial <- data.frame(
  sample_ID = rep(1:n_samples, each=sample_size),
  # value = runif(sample_size*n_samples, 0, 10)
  # value = rnorm(sample_size*n_samples, 0, 2)
  value = rexp(sample_size*n_samples, 2)
  )

df_trial |> group_by(sample_ID) |> 
  summarize(m = mean(value)) |> 
  ggplot() + geom_histogram(aes(x=m), bins=30, col='black', fill='skyblue')
```

*Looks familiar...*

## Magic of Central Limit Theorem {auto-animate="true"}

1.  Several samples
2.  Calculate mean ($\bar{X}$) in each sample
3.  Build the means distribution

**It is normally distributed!** *(almost)*

> If samples are small, distribution of means may not look like normal.

4.  Calculate resulting distribution's SD$^*$ and mean ( $\hat\mu$ )
5.  Obtained mean is "real mean" and SD is *Standard error*

# More magic!

## Standartization (or normalization..?)

Setting **mean=0 and sd=1**

::::: columns
::: {.column width="15%"}
$Z = \dfrac{\bar{X} - \mu}{SE}$
:::

::: {.column width="85%"}
```{r}
se = function(x){
  sd(x)/(sqrt(length(x)))
}
```

```{r}
sample_size = 100 
n_samples = 10000

df_trial <- tibble(
  sample_ID = rep(1:n_samples, each=sample_size),
  value = runif(sample_size*n_samples, 0, 10)
  )

df_standart = df_trial |> group_by(sample_ID) |> 
  summarize(m = mean(value)) |> 
  mutate(z_m = (m - mean(m))/sd(m)) 

 df_standart |> 
  ggplot() + 
  geom_density(aes(x=m, fill='Before'),
               # bins=30, col='black',
               # fill='skyblue'
               ) +
  geom_density(aes(x=z_m, fill='After'),
               # bins=30, col='black', 
               # fill='red3'
               ) +
  
   scale_fill_manual(values=c('Before'='skyblue', 'After'='red3'), 
                     name='Standartization')
```
:::
:::::

## Looks familiar? {auto-animate="true"}

::: fragment
**Three sigma rule!**
:::

```{r}
 df_standart |> 
  ggplot() + 
  geom_density(aes(x=z_m), fill='red3', alpha=.4
               # bins=30, col='black', 
               # fill='red3'
               ) +
  geom_vline(aes(xintercept = sd(z_m)), color='blue', lty=2, size=1.2) +
  geom_vline(aes(xintercept = -sd(z_m)), color='blue', lty=2, size=1.2) +
  geom_vline(aes(xintercept = 2*sd(z_m)), color='orange', lty=2, size=1.2) +
  geom_vline(aes(xintercept = -2*sd(z_m)), color='orange', lty=2, size=1.2) +
  geom_vline(aes(xintercept = 3*sd(z_m)), color='red', lty=2, size=1.2) +
  geom_vline(aes(xintercept = -3*sd(z_m)), color='red', lty=2, size=1.2) +
  xlab('Standartisized value')
```

## Three sigma rule! {auto-animate="true"}

::::: columns
::: {.column width="50%"}
We know that \~95% of the data lies between the two orange lines ($\mu_z \pm 2 \sigma$)

More precisely: $\mu_z \pm 1.96 \ \sigma$

**Here** $\sigma_z$ is SE = 1

$\mu_z = 0$ (standardisized mean of means)
:::

::: {.column .fade-in width="50%"}
Distribution of means

```{r}
 df_standart |> 
  ggplot() + 
  geom_density(aes(x=z_m), fill='red3', alpha=.4
               # bins=30, col='black', 
               # fill='red3'
               ) +
  geom_vline(aes(xintercept = sd(z_m)), color='blue', lty=2, size=1.5) +
  geom_vline(aes(xintercept = -sd(z_m)), color='blue', lty=2, size=1.5) +
  geom_vline(aes(xintercept = 2*sd(z_m)), color='orange', lty=2, size=1.5) +
  geom_vline(aes(xintercept = -2*sd(z_m)), color='orange', lty=2, size=1.5) +
  geom_vline(aes(xintercept = 3*sd(z_m)), color='red', lty=2, size=1.5) +
  geom_vline(aes(xintercept = -3*sd(z_m)), color='red', lty=2, size=1.5) +
  xlab('Standartisized value')
```
:::
:::::

## Three sigma rule --\> confidence interval {auto-animate="true"}

::::: columns
::: {.column width="50%"}
Thus, with probability of 0.95

$$ -1.96 \sigma_z \leq \dfrac{\bar{X} - \mu}{SE} \leq 1.96 \sigma_z $$

$$-1.96 \leq \dfrac{\bar{X} - \mu}{SE} \leq 1.96$$

$$-1.96 \ SE \leq \bar{X} - \mu \leq 1.96 \ SE $$

$$\bar{X} - 1.96 \ SE \leq \mu \leq  \bar{X} + 1.96 \ SE $$
:::

::: {.column .fade-in width="50%"}
Distribution of means

```{r}
 df_standart |> 
  ggplot() + 
  geom_density(aes(x=z_m), fill='red3', alpha=.4
               # bins=30, col='black', 
               # fill='red3'
               ) +
  geom_vline(aes(xintercept = sd(z_m)), color='blue', lty=2, size=1.5) +
  geom_vline(aes(xintercept = -sd(z_m)), color='blue', lty=2, size=1.5) +
  geom_vline(aes(xintercept = 2*sd(z_m)), color='orange', lty=2, size=1.5) +
  geom_vline(aes(xintercept = -2*sd(z_m)), color='orange', lty=2, size=1.5) +
  geom_vline(aes(xintercept = 3*sd(z_m)), color='red', lty=2, size=1.5) +
  geom_vline(aes(xintercept = -3*sd(z_m)), color='red', lty=2, size=1.5) +
  xlab('Standartisized value')
```
:::
:::::

## Confidence interval {auto-animate="true"}

```{css}
.transparent-text {
  color: rgba(0, 0, 0, 0.4);
}
```

::::: columns
::: {.column .transparent-text width="45%"}
Thus, with probability of 0.95

$$ -1.96 \sigma_z \leq \dfrac{\bar{X} - \mu}{SE} \leq 1.96 \sigma_z $$

$$-1.96 \leq \dfrac{\bar{X} - \mu}{SE} \leq 1.96$$

$$-1.96 \ SE \leq \bar{X} - \mu \leq 1.96 \ SE $$

$$\bar{X} - 1.96 \ SE \leq \mu \leq  \bar{X} + 1.96 \ SE $$
:::

::: {.column width="55%"}
We may never know the **true** value of the population mean ($\mu$), but we can estimate it from a sample and build an interval with a certain degree of *confidence* (e.g., 0.95)

$$ \bar X - x_{\alpha/2} * SE  \leq \mu_{true} \leq \bar X + x_{\alpha/2} * SE$$

-   $\alpha$ - Significance level (e.g. 0.05)

-   $1 - \alpha$ - Confidence level (e.g. 0.95)

-   $x_{\alpha/2}$ - values of $\alpha/2$ quantile in standart. normal distribution (mean=0, sd=1)

(e.g. 0.025 and 0.975 quantiles for $\alpha=0.05$)

((they are equal))
:::
:::::

#### Let's calculate it in R!

## What does confidence level mean?

::::: columns
::: {.column width="70%"}
Generating 1000 samples and 1000 95% CIs

```{r}
#| fig-height: 7
#| fig-width: 10

sample_size = 100
n_samples = 1000
true_mean=5


df_trial <- tibble(
  sample_ID = rep(1:n_samples, each=sample_size),
  value = rnorm(sample_size*n_samples, true_mean, 2)
  )

quantiles = qnorm(0, 1, p = c(0.025, 0.975))

df_ci = df_trial |> group_by(sample_ID) |> 
  summarize(mu = mean(value), 
            se_sample = se(value), 
            conf.min = mu + quantiles[1] * se_sample,
            conf.max = mu + quantiles[2] * se_sample) |> 
  mutate(out = ifelse((true_mean < conf.max) & (true_mean > conf.min), 'ok', 'missed'))

df_ci |>  
  ggplot(aes(y=sample_ID)) +
  geom_errorbar(aes(xmin = conf.min, xmax = conf.max, col=out)) +
  geom_vline(aes(xintercept = 5), col='red', lty=2, size=2) +
  labs(y="Sample", color='CI covers true mean') 
  
  

```
:::

::: {.column width="30%"}
How many CIs do cover true mean?

```{r}
table(df_ci$out)
```

**So, 95% of new generated CIs will cover true mean**

*Try to change size of samples*
:::
:::::

# Student for students

## Student's (t-) distribution {auto-animate="true"}

Comparing to **standard (!) normal distribution,** t-distribution have *heavier* tails.

::: {.fragment .fade-in}
It means that t-distribution's quantiles have bigger values comparing to normal
:::

![](/images/t_vs_z.jpg){fig-align="center" width="1000"}

## Student's (t-) distribution {auto-animate="true"}

Comparing to **standard (!) normal distribution,** t-distribution have *heavier* tails.

It means that t-distribution's quantiles have bigger values comparing to normal

*Standard Normal*:

```{r, echo = T, class="large-code"}
qnorm(c(0.025, 0.975), mean=0, sd=1)
```

*Student df=13*:

```{r, echo = T, class="large-code"}
qt(c(0.025, 0.975), df=13)
```

*Student df=50*:

```{r, echo = T, class="large-code"}
qt(c(0.025, 0.975), df=50)
```

## Student's (t-) distribution {auto-animate="true"}

**Degrees of freedom (df)** - the only parameter of distribution!

It always have **mean = 0** and "width" defined by **df**

> **Degrees of freedom = N - 1**
>
> N - size of sample

## Confidence interval using quantiles from t-distribution {auto-animate="true"}

$$ \bar X - t_{\alpha/2} * SE  \leq \mu_{true} \leq \bar X + t_{\alpha/2} * SE$$

-   $\alpha$ - Significance level (e.g. 0.05)

-   $1 - \alpha$ - Confidence level (e.g. 0.95)

-   $t_{\alpha/2}$ - values of $\alpha/2$ quantile in **t-distribution**

## Confidence interval using quantiles from t-distribution {auto-animate="true"}

$$ \bar X - t_{\alpha/2} * SE  \leq \mu_{true} \leq \bar X + t_{\alpha/2} * SE$$

> Because of heavier tails, such CI can be more accurate for small $n$
>
> **And for large** $n$ **it looks like standard normal**

#### Let's calculate it in R!
