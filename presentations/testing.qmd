---
title: "Intro to testing"
format: 
  revealjs:
      center: true
      controls: true
      # controlsLayout: 'bottom-left'
      slide-number: true
      show-slide-number: all
      fontsize: 30px
      style: style.css
      progress: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.width = 8, fig.height = 4, dpi = 300)
library(dplyr)
library(tidyr)
library(ggplot2)

theme_custom <- theme(
    axis.text = element_text(size = 15),
    axis.title = element_text(size = 20),
    plot.title = element_text(size = 20, hjust = 0.5),
    plot.subtitle = element_text(size = 15, hjust = 0.5),
    legend.title = element_text(size = 15),
    legend.text = element_text(size = 15),
    panel.background = element_rect(fill = "white"), 
    panel.grid = element_blank()
  )

theme_set(theme_custom)
```

# Basics

```{css}
.faded-text {
  opacity: 0.4;
}
```

## Testing plan

1\) What we want to test? (**Hypotheses** about population)

::: {.fragment .semi-fade-out}
2\) Choose significance level

3\) Choose the test

4\) **Conduct the experiment**

5\) Conduct the test

6\) Conclusion
:::

## Hypotheses

In statistical testing, we will operate with **hypotheses.** *There are two basic hypotheses:*

-   **Null Hypothesis (**$H_0$**)** - There is **no effect** in population
-   **Alternative Hypothesis (**$H_A$**) -** There is **an effect** in population

> Usually we are interested in having an effect, as it means some discovery, and **rejecting null hypothesis**

## Testing plan

::: faded-text
1\) What we want to test? **Hypotheses**
:::

2\) Choose significance level

::: faded-text
3\) Choose the test

4\) **Conduct the experiment**

5\) Conduct the test

6\) Conclusion
:::

------------------------------------------------------------------------

::::: columns
::: {.column width="50%"}
### Type I error

Error of rejecting $H_0$ when $H_0$ is true
:::

::: {.column width="50%"}
### Type II error

Error of accepting $H_0$ when $H_0$ is false
:::
:::::

<br>

|                     | Reality: **True** | Reality: **False** |
|---------------------|:------------------|:-------------------|
| Measured: **True**  | Correct üòä        | Type 1 Error       |
| Measured: **False** | Type 2 Error      | Correct üòä         |

------------------------------------------------------------------------

Probability of Type I error ($\alpha$)

Probability of Type II error ($\beta$)

![](../images/typeserror.png){fig-align="center" width="600"}

## Significance level {auto-animate="true"}

Before testing we must define value $\alpha$ named **significance level**

**Significance level (**$\alpha$) - **predetermined threshold of rejecting** $H_0$.

> For example we set $\alpha = 0.05$ and got **p-value** \< $\alpha$:
>
> We reject the $H_0$ at the $\alpha = 0.05$ significance level, indicating that there is sufficient evidence to conclude that the observed result is unlikely to have occurred under the null hypothesis.

Thus we [**fix the Type 1 Error**]{.underline} , but [**Type 2 Error still can be large!**]{.underline}

The value $1-\beta$ called the [**power of the test**]{.underline}. *The smaller is the probability of type II error, the more powerful is the test.*

## Significance level {auto-animate="true"}

### Examples

-   **Medical/Pharmaceutical**: $\alpha = 0.01$ is common to avoid false positives.
-   **Psychology/Social**: Typically $\alpha = 0.05$.
-   **Economics/Business**: Sometimes $\alpha = 0.1$ is acceptable

## Testing plan

::: faded-text
1\) What we want to test? (**Hypotheses**)

2\) Choose significance level
:::

3\) Choose the test

-   Test's functionality

-   Test's assumptions

::: faded-text
4\) **Conduct the experiment**

5\) Conduct the test

6\) Conclusion
:::

## Test's functionality

**Does it test my hypothesis?**

**Is my hypothesis is the hypothesis this test checking?**

> I'm checking the difference between the average hemoglobin levels in sick and healthy people. T-test should be suitable for this

## Test's assumptions {auto-animate="true"}

**The conditions, the fulfillment of which ensures control over the quality of the test results**

**And the opposite is not true !!**

> For example, fulfillment of assumptions can guarantee fixed Type I error
>
> And if assumptions are violated, it doesn't mean Type I error will be big
>
> (but it still can be)

> T-test is good for big samples with distributions where average makes sense

## Test's assumptions {auto-animate="true"}

### Examples

+ The particular distribution of population (e.g. normal)

+ The equality of variances

+ The independence of groups

+ The type of data (categorical, numerical and etc.)

## One data, several hypothesis {auto-animate="true"}

::::: columns
::: {.column width="50%"}
1.  $X$ - Days in hospital

Divide patients into 2 groups by many days or few (e.g. \<\>15 days)

Red - with pneumonia

Blue - without pneumonia

$H_0$ - there's no association between pneumonia and days in the hospital

$\Rightarrow \chi^2$ test
:::

::: {.column width="50%"}
```{r}
sample_1 = c(rexp(200, 2), rnorm(100, 25, 2)) %>%  data.frame(x=.)
sample_2 = c(rexp(200, 1), rnorm(100, 15, 2)) %>%  data.frame(x=.)

odttg = ggplot() + 
  geom_histogram(data=sample_1, mapping = aes(x=x),
                 bins=50, alpha=0.4, fill='red', col='black') +
  geom_histogram(data=sample_2, mapping = aes(x=x), 
                 bins=50, alpha=0.4, fill='blue', col='black')
```

```{r}
#| fig-height: 6
#| fig-width: 8
odttg
```
:::
:::::

## One data, several hypothesis {auto-animate="true"}

::::: columns
::: {.column width="50%"}
2.  $X$ - Students' exam scores

Red - First group

Blue - Second group

$H_0$ - Score of students from **First group** and **Second group** are the same

$\Rightarrow$ Mann-Whitney test
:::

::: {.column width="50%"}
```{r}
#| fig-height: 6
#| fig-width: 8
odttg
```
:::
:::::

# Looking at the data and then choosing a test is the wrong approach!

# Sometimes there is no suitable test for your hypothesis and question

## Testing plan

::: faded-text
1\) What we want to test? (**Hypotheses**)

2\) Choose significance level

3\) Choose the test
:::

4\) **Conduct the experiment**

::: faded-text
5\) Conduct the test

6\) Conclusion
:::

## Testing plan

::: faded-text
1\) What we want to test? (**Hypotheses**)

2\) Choose significance level

3\) Choose the test

4\) **Conduct the experiment**
:::

5\) Conduct the test

::: faded-text
6\) Conclusion
:::

## Test result

The tests result in [**test statistic**]{.underline} and the [**p-value**]{.underline} .

-   Each test have its formula to calculate called [**test statistic**]{.underline}. This value has its own distribution (*e.g. t-distribution for the t-statistic from t-test*).

-   [**p-value**]{.underline} - the probability that, given a **true null hypothesis**, your observations will result **in such or more extreme value of test statistic**

> The smaller the p-value, the less likely we are to get our result under the null hypothesis, which states that there is no effect.

**The p-value does not indicate the magnitude of the effect!**

## "Pipeline"

![](/images/pvalpipeline.png){fig-align="center"}

## Parameters of test statistics' distributions

### [Degrees of freedom!]{.underline}

-   Often calculated based on [number of observations]{.underline}, groups and coefficients.

-   [Determines the shape of distribution!]{.underline}

> **-\> Amount of data affects testing results!**

#### Other parameters...

-   Mean, SD

-   Rate, shape

## t-distribution for different DFs

![](/images/dfsT.png){fig-align="center" width="600"}

```{r, warning=FALSE}
# t1 = rt(100000, 3)
# t2 = rt(100000, 3)
# t5 = rt(100000, 5)
# t10 = rt(100000, 10)
# t30 = rt(100000, 30)
# t100 = rt(100000, 100)
# 
# dft = data.frame(t1,t2,t5,t10,t30,t100) %>%  
#   pivot_longer(values_to = 'value', 
#                cols = 1:5, names_to = 't') %>% 
#   mutate(t = as.factor(t))
# 
# dft %>%  ggplot(aes(col=t, 
#                     x=value, 
#                     y=..density..)) + 
#   # geom_histogram(alpha=0.4, bins = 100) +
#   geom_density() +
#   xlim(c(-5,5))

```

## *p-value* calculation

![](../images/memes/p-value.jpg){fig-align="center"}

------------------------------------------------------------------------

### *p-value* $\geq \alpha$

Since the ***p-value*** greater than chosen significance level ($\alpha$) we **fail** to reject $H_0$

### *p-value* $< \alpha$

Since the ***p-value*** is less than chosen significance level ($\alpha$), we reject $H_0$ and conclude that there is sufficient statistical evidence to support $H_A$

------------------------------------------------------------------------

![](/images/memes/p_values_2x.png){fig-align="center" width="498"}

## One-sided hypothesis

$H_A: X_0 < X_1$

```{r}
plot(x=seq(-5,5,0.01), y=dt(seq(-5,5,0.01), df=19), 'l', lwd=2,
     main = 'T-distribution df=19',
     xlab='t', ylab='density')
abline(v=qt(0.95, df=19), col='red', lwd=2) 
text(x =3, y = 0.3, col='red',
     labels = paste('alpha = 0.05\nt =', qt(0.95, df=19) %>%round(2)), cex=1.3)

points(x = c(1.5, 3.1,-3.95), y = rep(0.03, 3), pch=c(13,19,13), cex=2)
text(x = c(1.1, 3.1,-3.95), y = rep(0.07, 3), 
     labels = c('t = 1.5', 't = 3.1', 't = -3.95'), cex=1.5)
```

```{r}
# plot(x=seq(0, 10,0.01), y=dchisq(seq(0,10,0.01), df=2), 'l', lwd=2,
#      main = 'Chi-squared distribution df=2', xlab='Chisq', ylab='density')
# abline(v=qchisq(0.95, df=2), col='red', lwd=2)
# text(x =7, y = 0.3, col='red',
#      labels = 'alpha = 0.05', cex=1)
# 
# points(x = c(2.1, 7.34), y = rep(0.03, 2), pch=c(13,19), cex=2)
# text(x = c(2.1, 7.34), y = rep(0.07, 3), 
#      labels = c('chisq. = 2.1', "chisq. = 7.34"), cex=1.2)
```

## Two-sided hypothesis

$H_A: X_0 \neq X_1$

```{r}
plot(x=seq(-5,5,0.01), y=dt(seq(-5,5,0.01), df=19), 'l', lwd=2,
     main = 'T-distribution df=19',
     xlab='t', ylab='density')
abline(v=qt(0.975, df=19), col='red', lwd=2) 
abline(v=qt(0.025, df=19), col='red', lwd=2)
text(x =c(-3,3), y = 0.3, col='red',
     labels = c(paste('alpha / 2\nt =', qt(0.025, df=19) %>% round(2)),
                paste('alpha / 2\nt =', qt(0.975, df=19) %>% round(2))), cex=1.3)


points(x = c(1.5, 3.1,-3.95), y = rep(0.03, 3), pch=c(13,19,19), cex=2)
text(x = c(1.2, 3.1,-3.95), y = rep(0.07, 3), 
     labels = c('t = 1.5', 't = 3.1', 't = -3.95'), cex=1.5)
```

## How to calculate critical value of test statistic?

*Example for T-distribution*

### One-sided hypotheses

::: large-code
```{r, echo=T}
qt(0.95, df=19)
qt(0.05, df=19)
```
:::

### Two-sided hypothesis

::: large-code
```{r, echo=T}
qt(0.975, df=19)
qt(0.025, df=19)
```
:::

------------------------------------------------------------------------

![](../images/t-table.png){fig-align="center" width="700"}

## Another pitfalls

1.  **Small sample size** or **Unbalanced groups**
    -   Power of tests will be reduced = less sensible results
2.  **Changing significance level**
    -   $\alpha$ must be defined **BEFORE** testing
3.  **Misinterpreting *p-values***
    -   It doesn't reject or accept $H_0$.

    -   **YOU reject or accept** $H_0$
4.  [**Multiple comparisons problem**]{.underline}

## Multiple comparisons {auto-animate="true"}

-   $1 - 0.05$ - probability of correctly rejected $H_0$

-   $(1-0.05)^n$ - probability of correctly rejected $\space H_0$ in $n$ tests

-   $1-(1-0.05)^n$ - probability of [**incorrectly rejected at least 1**]{.underline} $H_0$ in $n$ tests

$$
1 - (1 - 0.05) ^ {2} = 0.0975\\
1 - (1 - 0.05) ^ {5} = 0.2262191\\
1 - (1 - 0.05) ^ {20} = 0.6415141\\
1 - (1 - 0.05) ^ {100} = 0.9940795\\
$$

## Multiple comparisons {auto-animate="true"}

### Corrections!

Controlling the **FWER** - probability of making at least one Type I error.

-   Bonferroni

-   Holm-Bonferroni

-   ≈†id√°k

Controlling the **FDR** - expected proportion of false positives among all significant tests.

-   Benjamini-Hochberg

-   Benjamini-Yekutieli

# Let's go
