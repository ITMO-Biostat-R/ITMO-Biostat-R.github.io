---
title: "Statistics"
author: Vladislav Stanin  
date: 01/24/2024
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
```

# Hypotheses of means equality

# t-test

## Assumptions

-   [Variances of the two groups are equal]{.underline}
    -   Variances can not be equal when using **Welch t-test** (*default in R*)
-   Independence of collected observations
-   [*Samples should be large enough....*]{.underline}

## Hypotheses

-   **Null Hypothesis (**$H_0$**)**: The means of the two groups are equal.
-   **Alternative Hypothesis (**$H_A$**)**: The means of the two groups are different.
    -   Two-sided: "our" mean is either significantly greater than or less than the mean under the null hypothesis
    -   One-sided: "our" mean is significantly greater than the mean under the null hypothesis (instead of "greater" could be "less")

## Unpaired t-test [for independent samples]{.underline}

### Formula

Compare means of two groups: $t = \dfrac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$

Compare one group mean with a zero: $t = \dfrac{\bar{X}_1}{\sqrt{\frac{s_1^2}{n_1}}}$

-   $\bar{X_1}, \bar{X_2}$ - means of 1st and 2nd samples

-   $s_1, s_2$ - standard deviations of 1st and 2nd samples

-   $n_1, n_2$ -number of observations in samples

### Parameter of t-distribution

**Degrees of freedom (df)** - *number of independent piece of information*

-   **Equal to** number of observations - number of groups

    -   If two groups are compared: $df = N_1 + N_2 - 2$

    -   If group mean compared with zero: $df = N_1 - 1$

-   Calculated by *another big formula* for **Welch t-test**

### Does sunlight exposure affect plant height?

#### Data

```{r}
data_sunlight_plant = 
  data.frame(group = factor(rep(c("Sunlight", "Shade"), each = 30)),
             height = c(rnorm(30, mean = 15, sd = 2), 
                        rnorm(30, mean = 12, sd = 2)))

data_sunlight_plant %>% str() #!
```

```{r}
ggplant = data_sunlight_plant %>%  
  ggplot(aes(x=height,fill=group)) +
  theme_classic()

# ggplant + geom_boxplot(aes(y=group)) + theme(legend.position = "none")
ggplant + geom_density(alpha=0.5)
# ggplant + geom_histogram(alpha=0.8, binwidth=1, alpha=0.6, col='black')

```

#### Testing

**Example for one sample:**

```{r}
x = rnorm(10, mean=2, sd=2)
t.test(x)
```

**Example on two samples:**

```{r}
x1 = rnorm(10, mean=2, sd = 4)
x2 = rnorm(10, mean=-1, sd=2)
# boxplot(x1, x2, horizontal = T)
t.test(x1,x2)
```

[**Our *plant and sunlight* data:**]{.underline}

```{r}
# t.test(height ~ group, data = data_sunlight_plant, var.equal = TRUE) # Classic t-test
t.test(height ~ group, 
       data = data_sunlight_plant) # Welch t-test
```

-   **t-value** = -4.6261
-   **p-value** = 2.137e-05 = $2.137 * 10^{-5}$ = $0.00002137$
    -   $2.137 * 10^{-5} < 0.05 \leftrightarrow p.value < \alpha$
    -   We can with great confidence reject the $H_0$ and accept $H_A$ about difference of averages

### Comparison of basic and Welch t-test

|   | Basic t-test | Welch t-test |
|----|:--:|:--:|
| = N ; = SD ; = MEAN | **GOOD** | **GOOD** |
| = N ; $\neq$ SD ; = MEAN | **GOOD** | [**BETTER**]{.underline} |
| \> N ; \> SD ; = MEAN | [**UNSTABLE**]{.underline} | **GOOD** |
| \< N ; \> SD ; = MEAN | [**UNSTABLE**]{.underline} | **GOOD** |
| = N ; = SD ; $\neq$ MEAN | **GOOD** | **GOOD** |
| = N ; $\neq$ SD ; $\neq$ MEAN | **GOOD** | **GOOD** |
| \> N ; \> SD ; $\neq$ MEAN | **OK** | **OK** |
| \< N ; \> SD ; $\neq$ MEAN | **OK** | **OK** |

## Non-Parametric Alternative [for independent samples]{.underline}

Since the t-test requires calculation of the mean, it is sensitive to outliers. Therefore, it is better to use a non-parametric approach

**Mann-Whitney test** **(also known as** **Wilcoxon** **rank-sum test**)

### Hypotheses

[**The test does not directly compare means or medians!**]{.underline}

-   **Null Hypothesis** ($H_0$​): Two populations (from which the samples are drawn) have the **same distribution** in terms of relative position.

-   **Alternative Hypothesis** ($H_A$​): Two populations have **different distributions** in terms of relative position (i.e., one distribution tends to yield larger values than the other).

If $X$ comes from population 1 and $Y$ comes from population 2, the null hypothesis often implies:

$H_0: P(X>Y)=0.5$

### Example

```{r}
sample1 = c(rnorm(20, 5, 2), 18)
sample2 = c(rnorm(20, 7, 2), 22)

data_sample = data.frame(group = rep(c("sample1", "sample2"), each=21),
                         value = c(sample1, sample2))
```

```{r}
data_sample %>%  ggplot(aes(x = value, fill=group)) + geom_density(alpha=0.5)
```

```{r wilcoxon}
t.test(value ~ group,
       data=data_sample)

wilcox.test(value ~ group,
            data=data_sample)
```

Base R only has `wilcox.test()`, however the documentation says Wilcoxon tests "*is also known as the Mann-Whitney test".*

## Paired t-test [for dependent samples]{.underline}

Used to compare two related groups **(e.g., before and after treatment).**

### Formula

$t = \dfrac{\bar{d}}{s_d / \sqrt{n}}$

-   where $\bar{d}$ is the average difference

    $\bar{d} = \dfrac{\sum^N_{i=1} X_{1i} - X_{2i}}{N} = \dfrac{\sum^N_{i=1} d_i}{N}$,

    -   $X_{1i}$ - $i$-th member of the 1st sample (**before treatment**)

    -   $X_{2i}$ - $i$-th member of the 2nd sample (**after treatment**)

-   and $s_d$ is the standard deviation of the differences.

    $s_d = \sqrt{\dfrac{\sum_{i=1}^N (d - \bar{d})^2}{N-1}}$

### Additional assumption

-   Normal distribution of differences ($d_i$)

### Parameter of t-distribution

*As two samples contain the same members,* $df$ **= number of members - 1**

### Is there a significant change in blood pressure before and after treatment?

#### Data

```{r}
before <- rnorm(20, mean = 120, sd = 10)
after <- before + rnorm(20, mean = -5, sd = 5)

data_pressure = data.frame(
 pressure = c(before, after),
 group = factor(rep(c('before', 'after'), each=20))
)

data_pressure %>% head() 
```

```{r paired-t-test-visualization}
ggpress = data_pressure %>%  
  ggplot(aes(x=pressure, fill=group)) +
  theme_classic()

ggpress + geom_boxplot(aes(y=group)) + theme(legend.position = "none")
# ggplant + geom_density(alpha=0.5)
# ggplant + geom_histogram(alpha=0.8, binwidth=1)
```

#### Testing

```{r paired-t-test}
t.test(pressure ~ group,
       data = data_pressure,
       paired = TRUE) # <---- !!!
```

## Non-Parametric Alternative [for dependent samples]{.underline}

**Wilcoxon signed-rank test**

### Hypotheses

[**Testing medians, not means**]{.underline}

-   **Null Hypothesis** ($H_0$​): Two dependent populations (from which the samples are drawn) have the **same distribution** in terms of relative position.

-   **Alternative Hypothesis** ($H_A$​): Two dependent populations have **different distributions** in terms of relative position (i.e., one distribution tends to yield larger values than the other).

### Testing before/after data

```{r}
sample_before = c(rnorm(20, 5, 2), 18)
sample_after = sample_before + rnorm(21, 1, 2)

data_before_after = data.frame(group = rep(c("before", "after"), each=21),
                         value = c(sample_before, sample_after))
```

```{r}
data_before_after %>%  ggplot(aes(x = value, fill=group)) + geom_density(alpha=0.5)
```

```{r wilcoxon-paired}
wilcox.test(value ~ group,
            data=data_before_after,
            paired = TRUE) ## <---
```

# Categorical data testing

# Chi-Square ($\chi^2$) Test ([of independence]{.underline})

Used to test relationships between categorical variables.

## Assumptions

-   Frequencies in each cell are sufficiently large **(\> 5)**.

## Hypotheses

-   **Null Hypothesis (**$H_0$**)**: Two categorical variables are **independent**
-   **Alternative Hypothesis (**$H_A$**)**: Two categorical variables are **dependent**

## Formula

$\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$ where ($O_i$) are the observed frequencies, and ($E_i$) are the expected frequencies.

## Parameter of $\chi^2$-distribution

Degrees of freedom $df$

-   (number of **rows** in contingency table - 1) \* (number of **columns** in contingency table - 1)

## Contingency tables

### 2 x 2

#### Observed

|           | Cured | Non-cured |
|-----------|-------|-----------|
| Treatment | 20    | 7         |
| Placebo   | 9     | 18        |

#### Expected

|           | Cured | Non-cured |
|-----------|-------|-----------|
| Treatment | 14.5  | 12.5      |
| Placebo   | 14.5  | 12.5      |

$\chi^2 = \sum^2_i \sum^2_j \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ ($i$ - row, $j$ - column) $\rightarrow \chi^2 = \frac{(20-14.5)^2}{14.5} + \frac{(9-14.5)^2}{14.5} + \frac{(18-14.5)^2}{12.5}+\frac{(7-14.5)^2}{12.5}$

$df = (2-1)*(2-1) = 1$

### N X M

#### Observed

+-----------------+-----------------+--------------------+---------------+
| Blood Pressure  | **Underweight\  | **Normal Weight\   | **Overweight\ |
|                 | (BMI \< 18.5)** | (BMI 18.5--24.9)** | (BMI ≥ 25)**  |
+-----------------+-----------------+--------------------+---------------+
| Normal          | 40              | 120                | 90            |
+-----------------+-----------------+--------------------+---------------+
| Prehypertension | 20              | 80                 | 130           |
+-----------------+-----------------+--------------------+---------------+
| Hypertension    | 10              | 50                 | 160           |
+-----------------+-----------------+--------------------+---------------+

#### Expected

+-----------------+-----------------+--------------------+---------------+
| Blood Pressure  | **Underweight\  | **Normal Weight\   | **Overweight\ |
|                 | (BMI \< 18.5)** | (BMI 18.5--24.9)** | (BMI ≥ 25)**  |
+-----------------+-----------------+--------------------+---------------+
| Normal          | 25              | 89.29              | 135.71        |
+-----------------+-----------------+--------------------+---------------+
| Prehypertension | 23              | 82.14              | 124.86        |
+-----------------+-----------------+--------------------+---------------+
| Hypertension    | 22              | 78.57              | 119.43        |
+-----------------+-----------------+--------------------+---------------+

$\chi^2 = \sum^2_i \sum^2_j \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ ($i$ - row, $j$ - column)

$df = (N-1)(M-1)$

## Does disease outcome depend on treatment group ?

+----------------------------------+-------------+-------------+---------------------------+
|                                  | Cured       | Not cured   | [*Row Total*]{.underline} |
+==================================+=============+=============+===========================+
| **Treatment**                    | 12          | 8           | 20                        |
+----------------------------------+-------------+-------------+---------------------------+
| **Placebo**                      | 6           | 10          | 16                        |
+----------------------------------+-------------+-------------+---------------------------+
| [***Column Total***]{.underline} | 18          | 18          | [**36**]{.underline}      |
+----------------------------------+-------------+-------------+---------------------------+

### Expected values calculation

In a 2x2 contingency table, the expected frequency for each cell is calculated as:

$$E_{ij} = \frac{(R_i \times C_j)}{N}$$

Where: - $R_i$ = Total of row $i$. - $C_j$ = Total of column $j$. - $N$ = Grand total.

|               | Cured                    | Not cured                |
|---------------|--------------------------|--------------------------|
| **Treatment** | $(18 \times 20)/36 = 10$ | $(18 \times 20)/36 = 10$ |
| **Placebo**   | $(18 \times 16)/36 = 8$  | $(18 \times 16)/36 = 8$  |

### Statistic calculation

$\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$ where ($O_i$) are the observed frequencies, and ($E_i$) are the expected

### Code

```{r}
data_cure = data.frame(
  Outcome = factor(rep(c('Cured', 'Not cured'), e=18)),
  Group = factor(c(rep("Treatment", 12), rep("Placebo", 6), rep("Treatment", 8), rep("Placebo", 10))))

data_cure %>%  head()
```

### Contingency table

```{r}
contingency_table = table(data_cure$Outcome, data_cure$Group)
contingency_table
```

### Test

We don't need to calculate the expected values. The `chisq.test()` do it for us.

```{r}
chisq.test(contingency_table, 
           # correct = F
           )
```

-   $p.value > 0.05$

-   We **failed to reject** the $H_0$ about independence of `Outcome` and `Group`

# Fisher's Exact Test

> Used for [**small sample sizes** where the Chi-Square test assumptions do not hold.]{.underline}

-   Usually used **only for 2x2 contingency table**
-   The test is **exact** because it computes the exact probability of the observed data (and more extreme data) under the null hypothesis

## Hypotheses

-   **Null Hypothesis (**$H_0$**):** Two categorical variables are **independent**
-   **Alternative Hypothesis (**$H_A$**): Two categorical variables are dependent**

## Does the presence of the disease depend on the gene knockout?

### Data

|   | Disease | No disease | [*Row Total*]{.underline} |
|----|----|----|----|
| **Gene knockout** | [**1**]{.underline} | [**3**]{.underline} | 4 |
| **Control** | [**4**]{.underline} | [**2**]{.underline} | 6 |
| [***Column Total***]{.underline} | 5 | 5 | 10 |

```{r}
data_knockout = data.frame(
  Outcome = factor(rep(c('Disease', 'No disease'), e=5)),
  Group = factor(c("Knockout", rep("Control", 4), rep("Knockout", 3), rep("Control", 2))))

data_knockout %>%  head()
```

### Contingency table

```{r}
contingency_table_1 = table(data_knockout$Group, data_knockout$Outcome)
contingency_table_1
```

### Testing

```{r fisher}
fisher.test(contingency_table_1)
```

-   $p.value > 0.05$
-   We failed reject $H_0$ about absence of association between categorical variables.

# Correlation analysis

Correlation analysis measures the **strength** and **direction** of a linear relationship between two variables. The most commonly used correlation measure is the **Pearson's correlation coefficient** which indicates [**linear**]{.underline} **relationship**

## Formula

-   Pearson's correlation coefficient:

    $\rho = \dfrac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum (X_i - \bar{X})^2 \sum (Y_i - \bar{Y})^2}} \in [-1;1]$

### Interpretation

-   $−1$ indicates a perfect negative (inverse) linear relationship.

-   $+1$ indicates a perfect positive linear relationship.

-   $0$ indicates no linear relationship.

## Is there a relationship between temperature and metabolic rate?

### Data

```{r}
plant_height_df = data.frame(
  # Sunlight_hrs_per_day = c(4,5,5,6,6,7,8,8,9,10),
  # Plant_height = c(10.2, 11.5, 11.9, 13.4, 13.8, 14, 15, 16.2, 17.8, 18.3)
  Sunlight_hrs_per_day = rep(4:10, e=4),
  Plant_height = rep(4:10, e=4) + rnorm(28,0,2)
)

head(plant_height_df)
```

```{r}
plot(x = plant_height_df$Sunlight_hrs_per_day, 
     y=plant_height_df$Plant_height, 
     main = NULL, 
     xlab = "Sunlight (hrs/day)", 
     ylab = "Plant height", pch=19, col='blue')
```

### Calculation

-   `corr()` - calculates correlation matrix between all features of dataframe

```{r}
cor(plant_height_df)
# cor(iris %>%select(where(is.numeric)))
```

-   `corr.test()` - also tests how far the coefficient is from zero *(t-test)*

```{r}
cor.test(plant_height_df$Sunlight_hrs_per_day, 
         plant_height_df$Plant_height)
```

## Example of 0 correlation

```{r}
x1=rnorm(500)
y1=rnorm(500)
plot(x1,y1)
```

```{r}
cor.test(x1,y1)
```

## Correlation does not imply causation

Website with numerous [spurious correlations](https://tylervigen.com/spurious-correlations)

# Rank correlations

> [The problems with Pearson correlation are that it is not suitable for "strange" non-linear relationships, non-normally distributed traits and data with outliers.]{.underline}

## Spearman correlation

-   Suitable for non-linear relationships

-   Less sensitive to outliers

-   Calculated based on ranks

## Examples

### Non-linear

```{r}
x = runif(50, -10, 10)
y = (x + rnorm(50, 0, 2))^3
plot(x,y, pch=19)
```

```{r}
cor(x,y)
cor(x,y, method = "spearman")
# cor(x,y, method = "kendall")
```

### With outliers

```{r}
x = c(rnorm(20,0,3), c(15, 19))
y = c(x[1:20] + c(rnorm(20,0,1)), 0, 0)
plot(x,y, pch=19)
```

```{r}
cor(x,y)
cor(x,y, method = "spearman")
# cor(x,y, method = "kendall")
```

> By conducting correlation analysis, we only answered the question "Is there a significant relationship between the values?". Can we, using this knowledge, predict the values of one variable based on the second variable? **Yes!**

# Linear Regression

Used to predict a continuous outcome based on one *or more* predictors.

## Formula

$y = \beta_0 + \beta_1x_1 + ... + \beta_nx_n + \epsilon$

-   $y$ - dependent variable (response)

-   $x$ - independent variable (predictor)

-   $\beta_0$ - intercept - value of $y$, when $x=0$

-   $\beta_i$ - **coefficients** - values by which $y$ is changed when the corresponding $x_i$ is increased by 1.

-   $\epsilon$ - so-called **error -** differences between real $y$ values and esimated by model values

## Formula of model (linear)

$y_{pred} = \beta_0 + \beta_1x_1 + ... + \beta_nx_n$

## Assumptions

-   [**Homoscedasticity: Residuals have constant variance (\~)**]{.underline} - $\sigma^2(\epsilon) = const$
-   **Mean of residuals** is close to [**zero**]{.underline}
    -   ***Stricter assumption*****: residuals are normally distributed** around zero **(\~)** - $\epsilon \sim Norm(0, \sigma^2)$\
        This assumption is equivalent to normality of $y$
-   Observations $y_i$ are **independent** (not correlated) **(\~)**
-   **Linearity**: The relationship between variables is considered to be linear.

> **(\~) denotes assumptions needed for hypotheses testing and confidence estimation only**

# Simple linear regression

Linear regression model with **one dependent** $y$ (response) and one **independent** variable $x$ (predictor). Simple line which estimate linear relationship between $x$ and $y$.

## Formula of model

$y_{pred} = \beta_0 + \beta_1x_1$

## Can plant height be predicted based on soil nutrient levels?

### Data

```{r}
nutrients <- rnorm(50, mean = 10, sd = 3)
height <- 2 * nutrients + rnorm(50, mean = 0, sd = 5)

data_plant_nutrient <- data.frame(nutrients, height)
```

```{r}
plot(nutrients, height, main = "Nutrients vs Height",
     xlab = "Nutrients", ylab = "Height",
     pch=19)
```

### Model creation

```{r}
model <- lm(height ~ nutrients, data = data_plant_nutrient)
model
```

```{r}
plot(nutrients, height, main = "Nutrients vs Height",
     xlab = "Nutrients", ylab = "Height",
     pch=19)
abline(model, col = "red", lwd=3)
legend("topleft",
       legend = c("y_pred"),
       col = c('red'),
       bty = 'n', lwd=3)
```

# Model summary

```{r}
summary_model = summary(model)
summary_model
```

### Residuals

> Difference between values of observed data ($y_i$) and estimated by linear model values $y_{pred,\space i}$ for each $x_i$

-   $y_i-y_{pred,\space i}$

```{r, eval=F}
# model$residuals
residuals(model)
```

### Coefficients:

> Estimate the effect of $x$ on the value of $y$.

Their values are estimated to minimize $MSE = \sum (y_i - y_{pred,\space i})^2$ **(Squared sum of errors)**

```{r}
# model$coefficients
coefficients(model)
```

### p-values Pr(\>\|t\|): Test the null hypothesis that coefficients are zero (t-test).

$$t = \dfrac{Estimated\space coefficient}{Std. Error} = \dfrac{\beta_i}{\sqrt{\frac{\sigma_{residuals}}{\sum(x_i-\overline{x})^2}}} = \dfrac{\beta_i}{\sqrt{\frac{1}{n-2}*\frac{\sum(y_i-y_{pred,\space i})^2}{\sum(x_i-\overline{x})^2}}}$$

-   More informative for slope than for intercept

```{r}
# summary_model$coefficients[,3:4]
summary_model$coefficients
```

### Residual standard error

$$Residual\space Standard\space Error\space (RSE) = \sqrt{\dfrac{\sum (y_i - y_{pred})^2}{df}}$$

$df = n - p$, where $n$ - number of observations and $p$ is number of estimated parameters (in our case they are **slope** and **intercept).**

```{r}
summary_model$sigma
```

### R-squared

> Proportion of variance explained by the model

$$R^2 = 1 - \dfrac{\sum(y_i - y_{pred})^2}{\sum(y_i-\overline{y})^2} = 1-\dfrac{SSR}{SST}$$

-   For simple linear regression is equal to squared Pearson correlation coefficient $\rho^2$

```{r}
summary_model$r.squared
```

### Adjusted R-squared:

> [Penalizes for the number of predictors]{.underline} to avoid overestimating the model's explanatory power when additional (possibly irrelevant) predictors are added

$$\overline{R}^2 = 1 - \dfrac{\dfrac{\sum(y_i - y_{pred})^2}{n-p}}{\dfrac{\sum(y_i-\overline{y})^2}{n-1}} = 1-\dfrac{\dfrac{SSR}{n-p}}{\dfrac{SST}{n-1}}$$

$p$ - number of parameters, $n$ - number of observations

```{r}
summary_model$adj.r.squared
```

### F-statistic

> Tests the overall significance of the regression model.

$$F = \dfrac{\frac{\sum(\overline y - y_{pred,\space i})^2}{p-1}}{\frac{\sum(y_i-y_{pred,\space i})^2}{n-p}} = \dfrac{\frac{SSR}{p-1}}{\frac{SSE}{n-p}} = \dfrac{MSR}{MSE}$$

$p$ - number of parameters, $n$ - number of observations

-   ***p-values*** are calculated according to [**Fisher distribution**]{.underline}
-   [In simple linear regression, it is equivalent to the t-test for the slope.]{.underline}

```{r}
# pf(summary_model$fstatistic[1], summary_model$fstatistic[2], 
   # summary_model$fstatistic[3], lower.tail = FALSE)

summary_model$fstatistic
```

> ***A higher F-statistic indicates that the model's explained variability (MSR - Regression mean squares) is large relative to the unexplained variability (MSE - Mean of squares error), suggesting that the regression model is capturing a significant relationship.***

#### F-test

```{r}
anova(model)
```

# Multiple Linear Regression

[Several independent variables]{.underline} (predictors)

## Formula of model

$y_{pred} = \beta_0 + \beta_1x_1 + ... + \beta_nx_n$

## Adding one more predictor for plant height model

```{r}
sunlight_hrs = rep(5:9, each=10) + rnorm(50, 0, 1)
data_plant_nutrient = data_plant_nutrient %>% arrange(height) %>%  cbind(sunlight_hrs)
data_plant_nutrient %>% str()
```

```{r}
plot(data_plant_nutrient$height, data_plant_nutrient$sunlight_hrs, 
     main = "Sunlight vs Height",
     xlab = "Sunlight", ylab = "Height",
     pch=19)
```

### Additive

$$
y = \beta_0 + \beta_1x_1 + \beta_2x_2
$$

```{r}
model_mullti = lm(height ~ sunlight_hrs + nutrients, data = data_plant_nutrient)
summary(model_mullti)
```

### With interaction

$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2$$

**Now coefficient have different interpretation:**

-   $\beta_1$​: Effect of $x_1$​ on $y$ when $x_2=0$.
-   $\beta_2$​: Effect of $x_2$​ on $y$ when $x_1=0$.
-   $\beta_3$: **Interaction coefficient**, showing how the relationship between $x_1$​ and $y$ changes as $x_2$​ changes.

> You should include an interaction if you hypothesize that the relationship between one predictor and the dependent variable changes depending on the level of another predictor. For example, the effect of a drug might vary depending on the age of the patient.

```{r}
# model_mullti = lm(height ~ nutrients + sunlight_hrs + nutrients:sunlight_hrs, data = data_plant_nutrient)
model_mullti_interact = lm(height ~ nutrients * sunlight_hrs, data = data_plant_nutrient)
summary(model_mullti_interact)
```

-   `x1:x2` denotes interaction

-   `x1 * x2` is equivalent to `x1 + x2 + x1:x2`

## Testing if model is better with new predictors

```{r}
anova(model, model_mullti_interact)
```

-   $p.value < 0.05$ and it means that new model better reduce the residual variance

## Plot the line "without" modelling

```{r}
data_plant_nutrient %>% 
  ggplot(aes(x=nutrients, y=height)) +
  geom_point(col='blue', show.legend = T) +
  geom_smooth(formula = y ~ x, method = 'lm', show.legend = T) + ## <---- creates model and plots the line
  theme_bw()
```

# P.S.

## Is your data normal?

### QQ-plot

Draws the correlation between a given sample and the normal distribution

### On sunlight plant data

Analyzing each group independantly

```{r}
qqnorm(data_sunlight_plant %>%  filter(group=='Sunlight') %>%  pull(height))
qqline(data_sunlight_plant %>%  filter(group=='Sunlight') %>%  pull(height))

qqnorm(data_sunlight_plant %>%  filter(group=='Shade') %>%  pull(height))
qqline(data_sunlight_plant %>%  filter(group=='Shade') %>%  pull(height))

```

### On blood pressure data

```{r}
qqnorm(data_pressure %>%  filter(group=='before') %>%  pull(pressure))
qqline(data_pressure %>%  filter(group=='before') %>%  pull(pressure))

qqnorm(data_pressure %>%  filter(group=='after') %>%  pull(pressure))
qqline(data_pressure %>%  filter(group=='after') %>%  pull(pressure))
```

## Chi-Square Goodness-of-Fit Test

## N x 1 contingency table

*Below are two different tables, but they are placed next to each other*

| Observed | Expected |
|----------|----------|
| 92       | 90       |
| 27       | 30       |
| 28       | 30       |
| 17       | 10       |

$\chi^2 = \sum^4_i \frac{(O_i - E_i)^2}{E_i}$, $df = N-1$

## Are observed ratios of pea plant traits consistent with Mendel's expected 9:3:3:1 ratio?

#### Data

```{r}
observed <- c(435, 150, 160, 55)
expected <- c(9, 3, 3, 1) / sum(c(9, 3, 3, 1)) * sum(observed)

```

```{r}
barplot(rbind(observed, expected), 
        beside = TRUE, 
        col = c("blue", "red"), 
        legend = c("Observed", "Expected"))
```

#### Code

```{r}
chisq.test(x=observed, p=expected/sum(expected))
```

-   `p` - expected proportions (instead of values)

-   $p.value > 0.05$ and it means that the two sets of frequencies are likely to belong to the same distribution or in other words: the observed values are not significantly different from the expected values

### 
