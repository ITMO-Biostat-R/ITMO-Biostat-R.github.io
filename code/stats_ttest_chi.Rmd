---
title: "Statistics"
author: Vladislav Stanin  
date: 01/24/2024
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
```

# Hypotheses of means equality

# t-test

## Assumptions

-   [Variances of the two groups are equal]{.underline}
    -   Variances can not be equal when using **Welch t-test** *(default in R)*
-   Independence of collected observations
-   [*Samples should be large enough....*]{.underline} *or originated from normally distributed population*

## Hypotheses

-   **Null Hypothesis (**$H_0$**)**: The means of the two groups are equal.
-   **Alternative Hypothesis (**$H_A$**)**: The means of the two groups are different.
    -   Two-sided: "our" mean is either significantly greater than or less than the mean under the null hypothesis
    -   One-sided: "our" mean is significantly greater than the mean under the null hypothesis (instead of "greater" could be "less")

## Unpaired t-test [for independent samples]{.underline}

### Formula

Compare means of two groups: $t = \dfrac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$

Compare one group mean with a hypothesized average : $t = \dfrac{\bar{X}_1 - \mu}{\sqrt{\frac{s_1^2}{n_1}}}$

-   $\bar{X_1}, \bar{X_2}$ - means of 1st and 2nd samples

-   $s_1, s_2$ - standard deviations of 1st and 2nd samples

-   $n_1, n_2$ -number of observations in samples

-   $\mu$ - hypothesized average (often = 0)

### Parameter of t-distribution

**Degrees of freedom (df)** - *number of independent piece of information*

-   **Equal to** number of observations - number of groups

    -   If two groups are compared: $df = N_1 + N_2 - 2$

    -   If group mean compared with zero: $df = N_1 - 1$

-   Calculated by *another big formula* for **Welch t-test**

### Does sunlight exposure affect plant height?

#### Data

```{r}
data_sunlight_plant = 
  data.frame(group = factor(rep(c("Sunlight", "Shade"), each = 30)),
             height = c(rnorm(30, mean = 15, sd = 2), 
                        rnorm(30, mean = 12, sd = 2)))

data_sunlight_plant %>% str() #!
```

```{r}
ggplant = data_sunlight_plant %>%  
  ggplot(aes(x=height,fill=group)) +
  theme_classic()

ggplant + geom_boxplot(aes(y=group)) + theme(legend.position = "none")
#ggplant + geom_density(alpha=0.5)
#ggplant + geom_histogram(alpha=0.7, binwidth=1, col='black')

```

#### Testing

**Example for one sample:**

$H_0: \ \overline{X}=5$:

```{r}
x = rnorm(30, mean=2, sd=2)
hist(x)
abline(v = 5, lty=2, col='red', lwd=4)
```

```{r}
t.test(x, mu = 5)
```

**Example on two samples:**

```{r}
x1 = rnorm(30, mean=2, sd = 4)
x2 = rnorm(30, mean=-1, sd=2)
boxplot(x1, x2, horizontal = T)
```

```{r}
t.test(x1, x2)
```

[**Our *plant and sunlight* data:**]{.underline}

```{r}
# t.test(height ~ group, data = data_sunlight_plant, var.equal = TRUE) # Classic t-test
t.test(height ~ group, 
       data = data_sunlight_plant) # Welch t-test

t.test(data_sunlight_plant$height[data_sunlight_plant$group=='Sunlight'], 
       data_sunlight_plant$height[data_sunlight_plant$group=='Shade'])
```

-   **t-value** \< 0
-   **p-value** \<\< $\alpha$ = 0.05
    -   We can reject the $H_0$ and accept $H_A$ about difference of averages

### Comparison of basic and Welch t-test

|   | Basic t-test | Welch t-test |
|----|:--:|:--:|
| = N ; = SD ; = MEAN | **GOOD** | **GOOD** |
| = N ; $\neq$ SD ; = MEAN | **GOOD** | [**BETTER**]{.underline} |
| \> N ; \> SD ; = MEAN | [**UNSTABLE**]{.underline} | **GOOD** |
| \< N ; \> SD ; = MEAN | [**UNSTABLE**]{.underline} | **GOOD** |
| = N ; = SD ; $\neq$ MEAN | **GOOD** | **GOOD** |
| = N ; $\neq$ SD ; $\neq$ MEAN | **GOOD** | **GOOD** |
| \> N ; \> SD ; $\neq$ MEAN | **OK** | **OK** |
| \< N ; \> SD ; $\neq$ MEAN | **OK** | **OK** |

## Paired t-test [for dependent samples]{.underline}

Used to compare two related groups **(e.g., before and after treatment).**

### Formula

$t = \dfrac{\bar{d}}{s_d / \sqrt{n}}$

-   where $\bar{d}$ is the average difference

    $\bar{d} = \dfrac{\sum^N_{i=1} X_{1i} - X_{2i}}{N} = \dfrac{\sum^N_{i=1} d_i}{N}$,

    -   $X_{1i}$ - $i$-th member of the 1st sample (**before treatment**)

    -   $X_{2i}$ - $i$-th member of the 2nd sample (**after treatment**)

-   and $s_d$ is the standard deviation of the differences.

    $s_d = \sqrt{\dfrac{\sum_{i=1}^N (d - \bar{d})^2}{N-1}}$

### Additional assumption

-   Normal distribution of differences ($d_i$)

### Parameter of t-distribution

*As two samples contain the same members,* $df$ **= number of members - 1**

### Is there a significant change in blood pressure before and after treatment?

#### Data

```{r}
before <- rnorm(20, mean = 120, sd = 10)
after <- before + rnorm(20, mean = -5, sd = 5)

data_pressure = data.frame(
 pressure = c(before, after),
 group = factor(rep(c('before', 'after'), each=20))
)

data_pressure %>% head() 
```

```{r paired-t-test-visualization}
ggpress = data_pressure %>%  
  ggplot(aes(x=pressure, fill=group)) +
  theme_classic()

ggpress + geom_boxplot(aes(y=group)) + 
# ggplant + geom_density(alpha=0.5) + 
# ggplant + geom_histogram(alpha=0.8, binwidth=1) +
  theme(legend.position = "none")
```

#### Testing

```{r paired-t-test}
t.test(pressure ~ group,
       data = data_pressure, 
       pair = TRUE) # <---- !!!
```

# Non-Parametric Alternatives

## Non-Parametric Alternative [for independent samples]{.underline}

**If you have**

-   "Strange" distribution

-   "Outliers" strongly affecting the mean value

-   Rank/ordinal data

-   Strange scale (hard to interpret)

-   It does not matter how much greater one value is than another, but it is important that it is **consistently** greater

-   ~~Small sample~~ (t-test even better for small samples)

**then you can use non-parametric approaches**

### **Mann-Whitney test** **(also known as** **Wilcoxon** **rank-sum test**)

### Hypotheses

[**The test does not compare means or medians!!!**]{.underline}

-   **Null Hypothesis** ($H_0$​): Two populations (from which the samples are drawn) have the **same distribution** in terms of relative position.

-   **Alternative Hypothesis** ($H_A$​): Two populations have **different distributions** in terms of relative position (i.e., one distribution tends to yield larger values than the other).

If $X$ comes from population 1 and $Y$ comes from population 2, the null hypothesis often implies:

$H_0: P(X>Y)=0.5$

### Example

```{r}
sample1 = c(rnorm(20, 5, 2), 18)
sample2 = c(rnorm(20, 7, 2), 22)

data_sample = data.frame(group = rep(c("sample1", "sample2"), each=21),
                         value = c(sample1, sample2))
```

```{r}
data_sample %>%  
  ggplot(aes(x = value, fill=group)) + 
  geom_density(alpha=0.5) +
  theme_classic()
```

```{r wilcoxon}
t.test(value ~ group,
       data=data_sample)

wilcox.test(value ~ group,
            data=data_sample)
```

Base R only has `wilcox.test()`, however the documentation says Wilcoxon tests "*is also known as the Mann-Whitney test".*

## Non-Parametric Alternative [for dependent samples]{.underline}

**Wilcoxon signed-rank test**

### Hypotheses

[**Testing medians, not means**]{.underline}

-   **Null Hypothesis** ($H_0$​): Two dependent populations (from which the samples are drawn) have the **same distribution** in terms of relative position.

-   **Alternative Hypothesis** ($H_A$​): Two dependent populations have **different distributions** in terms of relative position (i.e., one distribution tends to yield larger values than the other).

### Testing before/after data

```{r}
sample_before = c(rnorm(20, 5, 2), 18)
sample_after = sample_before + rnorm(21, 1, 2)

data_before_after = data.frame(group = rep(c("before", "after"), each=21),
                         value = c(sample_before, sample_after))
```

```{r}
data_before_after %>%  ggplot(aes(x = value, fill=group)) + geom_density(alpha=0.5)
```

```{r wilcoxon-paired}
wilcox.test(value ~ group,
            data=data_before_after,
            pair = TRUE) ## <---
```

# Categorical data testing

# Chi-Square ($\chi^2$) Test ([of independence]{.underline})

Used to test relationships between categorical variables.

## Assumptions

-   [**At least one category should be random**]{.underline}

-   Better on big samples

## Hypotheses

-   **Null Hypothesis (**$H_0$**):** There is [**no association**]{.underline} between two categorical variables
-   **Alternative Hypothesis (**$H_A$**):** There is [**some association**]{.underline} between two categorical variables

## Formula

$\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$ where ($O_i$) are the observed frequencies, and ($E_i$) are the expected frequencies.

## Parameter of $\chi^2$-distribution

Degrees of freedom $df$

-   (number of **rows** in contingency table - 1) \* (number of **columns** in contingency table - 1)

## Contingency tables

### Does disease outcome depend on treatment group ?

|                                  | Cured | Not cured | [*Row Total*]{.underline} |
|----------------------------------|-------|-----------|---------------------------|
| **Treatment**                    | 12    | 8         | 20                        |
| **Placebo**                      | 6     | 10        | 16                        |
| [***Column Total***]{.underline} | 18    | 18        | [**36**]{.underline}      |

### Expected values calculation

In a 2x2 contingency table, the expected frequency for each cell is calculated as:

$$E_{ij} = \frac{(total \ Row_i \times total \ Column_j)}{N}$$

Where:

-   $total \ Row_i$ = Sum of row $i$

-   $total \ Column_j$ = Sum of column $j$

-   $N$ = Grand total

It is calculated like this to remain total sums of rows and columns the same, but make ratios between inner cells' values (Odds) equal

|   | Cured | Not cured | [*Row Total*]{.underline} |
|----|----|----|----|
| **Treatment** | $(18 \times 20)/36 = 10$ | $(18 \times 20)/36 = 10$ | 20 |
| **Placebo** | $(18 \times 16)/36 = 8$ | $(18 \times 16)/36 = 8$ | 16 |
| [***Column Total***]{.underline} | 18 | 18 | [**36**]{.underline} |

### Statistic calculation

$\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$ where ($O_i$) are the observed frequencies, and ($E_i$) are the expected

### 2 x 2

#### Observed

|           | Cured | Non-cured |
|-----------|-------|-----------|
| Treatment | 20    | 7         |
| Placebo   | 9     | 18        |

#### Expected

|           | Cured | Non-cured |
|-----------|-------|-----------|
| Treatment | 14.5  | 12.5      |
| Placebo   | 14.5  | 12.5      |

$\chi^2 = \sum^2_i \sum^2_j \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ ($i$ - row, $j$ - column) $\rightarrow \chi^2 = \frac{(20-14.5)^2}{14.5} + \frac{(9-14.5)^2}{14.5} + \frac{(18-14.5)^2}{12.5}+\frac{(7-14.5)^2}{12.5}$

$df = (2-1)*(2-1) = 1$

### N X M

#### Observed

+-----------------+-----------------+--------------------+---------------+
| Blood Pressure  | **Underweight\  | **Normal Weight\   | **Overweight\ |
|                 | (BMI \< 18.5)** | (BMI 18.5--24.9)** | (BMI ≥ 25)**  |
+-----------------+-----------------+--------------------+---------------+
| Normal          | 40              | 120                | 90            |
+-----------------+-----------------+--------------------+---------------+
| Prehypertension | 20              | 80                 | 130           |
+-----------------+-----------------+--------------------+---------------+
| Hypertension    | 10              | 50                 | 160           |
+-----------------+-----------------+--------------------+---------------+

#### Expected

+-----------------+-----------------+--------------------+---------------+
| Blood Pressure  | **Underweight\  | **Normal Weight\   | **Overweight\ |
|                 | (BMI \< 18.5)** | (BMI 18.5--24.9)** | (BMI ≥ 25)**  |
+-----------------+-----------------+--------------------+---------------+
| Normal          | 25              | 89.29              | 135.71        |
+-----------------+-----------------+--------------------+---------------+
| Prehypertension | 23              | 82.14              | 124.86        |
+-----------------+-----------------+--------------------+---------------+
| Hypertension    | 22              | 78.57              | 119.43        |
+-----------------+-----------------+--------------------+---------------+

$\chi^2 = \sum^2_i \sum^2_j \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ ($i$ - row, $j$ - column)

$df = (N-1)(M-1)$

### Code

```{r}
data_cure = data.frame(
  Outcome = factor(rep(c('Cured', 'Not cured'), e=18)),
  Group = factor(c(rep("Treatment", 12), rep("Placebo", 6), rep("Treatment", 8), rep("Placebo", 10))))

data_cure %>%  head()
```

### Contingency table

```{r}
contingency_table = table(data_cure$Outcome, data_cure$Group)
contingency_table
```

### Plot

```{r}
mosaicplot(contingency_table)
```

### Test

We don't need to calculate the expected values. The `chisq.test()` do it for us.

```{r}
chisq.test(contingency_table, 
           # correct = FALSE
           )
```

-   $p.value > 0.05$

-   We **failed to reject** the $H_0$ about independence of `Outcome` and `Group`

# Paired categorical data testing (for dependent samples)

## McNemar's test

For example, comparing two COVID tests. Both tests either detect the virus or not in the [**same patients**]{.underline}.

## Assumptions

-   Better on big samples

## Hypotheses

-   **Null Hypothesis (**$H_0$**):** Sums of rows and columns are equal (both tests detect COVID similarly)
    -   a+b = a+c
    -   c+d = b+d
    -   **=\> b=c**
-   **Alternative Hypothesis (**$H_A$**):** Sums of rows and columns are [**NOT**]{.underline} equal

|   | **Test2: +** | **Test2: -** | [*Row Total*]{.underline} |
|----|----|----|----|
| **Test1: +** | a | c | a+c |
| **Test1: -** | b | d | b+d |
| [***Column Total***]{.underline} | a+b | c+d | **a+b+c+d** |

$\chi^2 = \dfrac{(b-c)^2}{b+c}$

## COVID Tests

### Data

Same patients!

|   | **Test2: +** | **Test2: -** | [*Row Total*]{.underline} |
|----|----|----|----|
| **Test1: +** | 12 | 8 | 20 |
| **Test1: -** | 6 | 10 | 16 |
| [***Column Total***]{.underline} | 18 | 18 | **36** |

### Code

```{r}
data_covid = data.frame(
  test1 = factor(rep(c('test2+', 'test2-'), e=18)),
  test2 = factor(c(rep("test1+", 12), rep("test1-", 6), rep("test1+", 8), rep("test1-", 10))))

data_covid %>%  head()
```

### Contingency table

```{r}
contingency_table = table(data_covid$test2, data_covid$test1)
contingency_table
```

### Testing

```{r}
mcnemar.test(contingency_table)
```

-   $p.value > 0.05$

-   We **failed to reject** the $H_0$ that independence of `Outcome` and `Group`

# Fisher's Exact Test

## Assumptions

-   **for [both not random]{.underline} categories! (**otherwise it will be too conservative)
    -   For example, we have four cups, and milk was added to some of them after the tea, and to others, milk was added first, and then tea. We invite an expert who has to guess which cup is which.

        |              | expert: Tea -\> Milk | expert: Tea -\> Milk |
        |--------------|----------------------|----------------------|
        | Tea -\> Milk | 3                    | 1                    |
        | Milk -\> Tea | 1                    | 3                    |

Usually used **for 2x2 contingency table**

The test is **exact** because it computes the exact probability of the observed data (and more extreme data) under the null hypothesis

## Hypotheses

-   **Null Hypothesis (**$H_0$**):** There is [**no association**]{.underline} between two categorical variables
    -   Odds ratio (OR) = 1
-   **Alternative Hypothesis (**$H_A$**):** There is [**some association**]{.underline} between two categorical variables
    -   Odds ratio (OR) $\neq$ 1

## Does the presence of the disease depend on the gene knockout?

### Data

|   | Expert guessed "treatment" | Expert guessed "control" | [*Row Total*]{.underline} |
|----|----|----|----|
| **Treatment** | [**1**]{.underline} | [**3**]{.underline} | 4 |
| **Control** | [**3**]{.underline} | [**1**]{.underline} | 4 |
| [***Column Total***]{.underline} | 4 | 4 | 8 |

```{r}
data_knockout = data.frame(
  Expert = factor(rep(c('guessed treatment', 'guessed placebo'), e=4)),
  Group = factor(c("Treatment", "Control", rep("Treatment", 3), rep("Control", 3))))

data_knockout %>%  head()
```

### Contingency table

```{r}
contingency_table_1 = table(data_knockout$Group, data_knockout$Expert)
contingency_table_1
```

### Testing

```{r fisher}
fisher.test(contingency_table_1)
```

-   $p.value > 0.05$
-   We failed reject $H_0$ about absence of association between categorical variables.

# P.S.

## Is your data normal?

### QQ-plot

Draws the correlation between a given sample and the normal distribution

### On sunlight plant data

Analyzing each group independantly

```{r}
qqnorm(data_sunlight_plant %>%  filter(group=='Sunlight') %>%  pull(height))
qqline(data_sunlight_plant %>%  filter(group=='Sunlight') %>%  pull(height))

qqnorm(data_sunlight_plant %>%  filter(group=='Shade') %>%  pull(height))
qqline(data_sunlight_plant %>%  filter(group=='Shade') %>%  pull(height))

```

### On blood pressure data

```{r}
qqnorm(data_pressure %>%  filter(group=='before') %>%  pull(pressure))
qqline(data_pressure %>%  filter(group=='before') %>%  pull(pressure))

qqnorm(data_pressure %>%  filter(group=='after') %>%  pull(pressure))
qqline(data_pressure %>%  filter(group=='after') %>%  pull(pressure))
```

## Chi-Square Goodness-of-Fit Test

## N x 1 contingency table

*Below are two different tables, but they are placed next to each other*

| Observed | Expected |
|----------|----------|
| 92       | 90       |
| 27       | 30       |
| 28       | 30       |
| 17       | 10       |

$\chi^2 = \sum^4_i \frac{(O_i - E_i)^2}{E_i}$, $df = N-1$

## Are observed ratios of pea plant traits consistent with Mendel's expected 9:3:3:1 ratio?

#### Data

```{r}
observed <- c(435, 150, 160, 55)
expected <- c(9, 3, 3, 1) / sum(c(9, 3, 3, 1)) * sum(observed)

```

```{r}
barplot(rbind(observed, expected), 
        beside = TRUE, 
        col = c("blue", "red"), 
        legend = c("Observed", "Expected"))
```

#### Code

```{r}
chisq.test(x=observed, p=expected/sum(expected))
```

-   `p` - expected proportions (instead of values)

-   $p.value > 0.05 \ (\alpha)$ and it means that the two sets of frequencies are likely to belong to the same distribution or in other words: the observed values are not significantly different from the expected values
