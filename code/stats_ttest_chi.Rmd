---
title: "Statistics"
author: Vladislav Stanin  
date: 01/24/2024
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
```

# Hypotheses of means equality

# t-test

## Assumptions

-   [Variances of the two groups are equal]{.underline}
    -   Variances can not be equal when using **Welch t-test** (*default in R*)
-   Independence of collected observations
-   [*Samples should be large enough....*]{.underline}

## Hypotheses

-   **Null Hypothesis (**$H_0$**)**: The means of the two groups are equal.
-   **Alternative Hypothesis (**$H_A$**)**: The means of the two groups are different.
    -   Two-sided: "our" mean is either significantly greater than or less than the mean under the null hypothesis
    -   One-sided: "our" mean is significantly greater than the mean under the null hypothesis (instead of "greater" could be "less")

## Unpaired t-test [for independent samples]{.underline}

### Formula

Compare means of two groups: $t = \dfrac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$

Compare one group mean with a zero: $t = \dfrac{\bar{X}_1}{\sqrt{\frac{s_1^2}{n_1}}}$

-   $\bar{X_1}, \bar{X_2}$ - means of 1st and 2nd samples

-   $s_1, s_2$ - standard deviations of 1st and 2nd samples

-   $n_1, n_2$ -number of observations in samples

### Parameter of t-distribution

**Degrees of freedom (df)** - *number of independent piece of information*

-   **Equal to** number of observations - number of groups

    -   If two groups are compared: $df = N_1 + N_2 - 2$

    -   If group mean compared with zero: $df = N_1 - 1$

-   Calculated by *another big formula* for **Welch t-test**

### Does sunlight exposure affect plant height?

#### Data

```{r}
data_sunlight_plant = 
  data.frame(group = factor(rep(c("Sunlight", "Shade"), each = 30)),
             height = c(rnorm(30, mean = 15, sd = 2), 
                        rnorm(30, mean = 12, sd = 2)))

data_sunlight_plant %>% str() #!
```

```{r}
ggplant = data_sunlight_plant %>%  
  ggplot(aes(x=height,fill=group)) +
  theme_classic()

# ggplant + geom_boxplot(aes(y=group)) + theme(legend.position = "none")
ggplant + geom_density(alpha=0.5)
# ggplant + geom_histogram(alpha=0.8, binwidth=1, alpha=0.6, col='black')

```

#### Testing

**Example for one sample:**

```{r}
x = rnorm(10, mean=2, sd=2)
t.test(x)
```

**Example on two samples:**

```{r}
x1 = rnorm(10, mean=2, sd = 4)
x2 = rnorm(10, mean=-1, sd=2)
# boxplot(x1, x2, horizontal = T)
t.test(x1,x2)
```

[**Our *plant and sunlight* data:**]{.underline}

```{r}
# t.test(height ~ group, data = data_sunlight_plant, var.equal = TRUE) # Classic t-test
t.test(height ~ group, 
       data = data_sunlight_plant) # Welch t-test
```

-   **t-value** = -4.6261
-   **p-value** = 2.137e-05 = $2.137 * 10^{-5}$ = $0.00002137$
    -   $2.137 * 10^{-5} < 0.05 \leftrightarrow p.value < \alpha$
    -   We can with great confidence reject the $H_0$ and accept $H_A$ about difference of averages

### Comparison of basic and Welch t-test

|   | Basic t-test | Welch t-test |
|----|:--:|:--:|
| = N ; = SD ; = MEAN | **GOOD** | **GOOD** |
| = N ; $\neq$ SD ; = MEAN | **GOOD** | [**BETTER**]{.underline} |
| \> N ; \> SD ; = MEAN | [**UNSTABLE**]{.underline} | **GOOD** |
| \< N ; \> SD ; = MEAN | [**UNSTABLE**]{.underline} | **GOOD** |
| = N ; = SD ; $\neq$ MEAN | **GOOD** | **GOOD** |
| = N ; $\neq$ SD ; $\neq$ MEAN | **GOOD** | **GOOD** |
| \> N ; \> SD ; $\neq$ MEAN | **OK** | **OK** |
| \< N ; \> SD ; $\neq$ MEAN | **OK** | **OK** |

## Non-Parametric Alternative [for independent samples]{.underline}

Since the t-test requires calculation of the mean, it is sensitive to outliers. Therefore, it is better to use a non-parametric approach

**Mann-Whitney test** **(also known as** **Wilcoxon** **rank-sum test**)

### Hypotheses

[**The test does not directly compare means or medians!**]{.underline}

-   **Null Hypothesis** ($H_0$​): Two populations (from which the samples are drawn) have the **same distribution** in terms of relative position.

-   **Alternative Hypothesis** ($H_A$​): Two populations have **different distributions** in terms of relative position (i.e., one distribution tends to yield larger values than the other).

If $X$ comes from population 1 and $Y$ comes from population 2, the null hypothesis often implies:

$H_0: P(X>Y)=0.5$

### Example

```{r}
sample1 = c(rnorm(20, 5, 2), 18)
sample2 = c(rnorm(20, 7, 2), 22)

data_sample = data.frame(group = rep(c("sample1", "sample2"), each=21),
                         value = c(sample1, sample2))
```

```{r}
data_sample %>%  ggplot(aes(x = value, fill=group)) + geom_density(alpha=0.5)
```

```{r wilcoxon}
t.test(value ~ group,
       data=data_sample)

wilcox.test(value ~ group,
            data=data_sample)
```

Base R only has `wilcox.test()`, however the documentation says Wilcoxon tests "*is also known as the Mann-Whitney test".*

## Paired t-test [for dependent samples]{.underline}

Used to compare two related groups **(e.g., before and after treatment).**

### Formula

$t = \dfrac{\bar{d}}{s_d / \sqrt{n}}$

-   where $\bar{d}$ is the average difference

    $\bar{d} = \dfrac{\sum^N_{i=1} X_{1i} - X_{2i}}{N} = \dfrac{\sum^N_{i=1} d_i}{N}$,

    -   $X_{1i}$ - $i$-th member of the 1st sample (**before treatment**)

    -   $X_{2i}$ - $i$-th member of the 2nd sample (**after treatment**)

-   and $s_d$ is the standard deviation of the differences.

    $s_d = \sqrt{\dfrac{\sum_{i=1}^N (d - \bar{d})^2}{N-1}}$

### Additional assumption

-   Normal distribution of differences ($d_i$)

### Parameter of t-distribution

*As two samples contain the same members,* $df$ **= number of members - 1**

### Is there a significant change in blood pressure before and after treatment?

#### Data

```{r}
before <- rnorm(20, mean = 120, sd = 10)
after <- before + rnorm(20, mean = -5, sd = 5)

data_pressure = data.frame(
 pressure = c(before, after),
 group = factor(rep(c('before', 'after'), each=20))
)

data_pressure %>% head() 
```

```{r paired-t-test-visualization}
ggpress = data_pressure %>%  
  ggplot(aes(x=pressure, fill=group)) +
  theme_classic()

ggpress + geom_boxplot(aes(y=group)) + theme(legend.position = "none")
# ggplant + geom_density(alpha=0.5)
# ggplant + geom_histogram(alpha=0.8, binwidth=1)
```

#### Testing

```{r paired-t-test}
t.test(pressure ~ group,
       data = data_pressure, 
       pair = T) # <---- !!!
```

## Non-Parametric Alternative [for dependent samples]{.underline}

**Wilcoxon signed-rank test**

### Hypotheses

[**Testing medians, not means**]{.underline}

-   **Null Hypothesis** ($H_0$​): Two dependent populations (from which the samples are drawn) have the **same distribution** in terms of relative position.

-   **Alternative Hypothesis** ($H_A$​): Two dependent populations have **different distributions** in terms of relative position (i.e., one distribution tends to yield larger values than the other).

### Testing before/after data

```{r}
sample_before = c(rnorm(20, 5, 2), 18)
sample_after = sample_before + rnorm(21, 1, 2)

data_before_after = data.frame(group = rep(c("before", "after"), each=21),
                         value = c(sample_before, sample_after))
```

```{r}
data_before_after %>%  ggplot(aes(x = value, fill=group)) + geom_density(alpha=0.5)
```

```{r wilcoxon-paired}
wilcox.test(value ~ group,
            data=data_before_after,
            pair = TRUE) ## <---
```

# Categorical data testing

# Chi-Square ($\chi^2$) Test ([of independence]{.underline})

Used to test relationships between categorical variables.

## Assumptions

-   [**At least one category should be random**]{.underline}

-   Better on big samples

## Hypotheses

-   **Null Hypothesis (**$H_0$**):** There is [**no association**]{.underline} between two categorical variables
-   **Alternative Hypothesis (**$H_A$**):** There is [**some association**]{.underline} between two categorical variables

## Formula

$\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$ where ($O_i$) are the observed frequencies, and ($E_i$) are the expected frequencies.

## Parameter of $\chi^2$-distribution

Degrees of freedom $df$

-   (number of **rows** in contingency table - 1) \* (number of **columns** in contingency table - 1)

## Contingency tables

### 2 x 2

#### Observed

|           | Cured | Non-cured |
|-----------|-------|-----------|
| Treatment | 20    | 7         |
| Placebo   | 9     | 18        |

#### Expected

|           | Cured | Non-cured |
|-----------|-------|-----------|
| Treatment | 14.5  | 12.5      |
| Placebo   | 14.5  | 12.5      |

$\chi^2 = \sum^2_i \sum^2_j \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ ($i$ - row, $j$ - column) $\rightarrow \chi^2 = \frac{(20-14.5)^2}{14.5} + \frac{(9-14.5)^2}{14.5} + \frac{(18-14.5)^2}{12.5}+\frac{(7-14.5)^2}{12.5}$

$df = (2-1)*(2-1) = 1$

### N X M

#### Observed

+-----------------+-----------------+--------------------+---------------+
| Blood Pressure  | **Underweight\  | **Normal Weight\   | **Overweight\ |
|                 | (BMI \< 18.5)** | (BMI 18.5--24.9)** | (BMI ≥ 25)**  |
+-----------------+-----------------+--------------------+---------------+
| Normal          | 40              | 120                | 90            |
+-----------------+-----------------+--------------------+---------------+
| Prehypertension | 20              | 80                 | 130           |
+-----------------+-----------------+--------------------+---------------+
| Hypertension    | 10              | 50                 | 160           |
+-----------------+-----------------+--------------------+---------------+

#### Expected

+-----------------+-----------------+--------------------+---------------+
| Blood Pressure  | **Underweight\  | **Normal Weight\   | **Overweight\ |
|                 | (BMI \< 18.5)** | (BMI 18.5--24.9)** | (BMI ≥ 25)**  |
+-----------------+-----------------+--------------------+---------------+
| Normal          | 25              | 89.29              | 135.71        |
+-----------------+-----------------+--------------------+---------------+
| Prehypertension | 23              | 82.14              | 124.86        |
+-----------------+-----------------+--------------------+---------------+
| Hypertension    | 22              | 78.57              | 119.43        |
+-----------------+-----------------+--------------------+---------------+

$\chi^2 = \sum^2_i \sum^2_j \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ ($i$ - row, $j$ - column)

$df = (N-1)(M-1)$

## Does disease outcome depend on treatment group ?

|                                  | Cured | Not cured | [*Row Total*]{.underline} |
|----------------------------------|-------|-----------|---------------------------|
| **Treatment**                    | 12    | 8         | 20                        |
| **Placebo**                      | 6     | 10        | 16                        |
| [***Column Total***]{.underline} | 18    | 18        | [**36**]{.underline}      |

### Expected values calculation

In a 2x2 contingency table, the expected frequency for each cell is calculated as:

$$E_{ij} = \frac{(R_i \times C_j)}{N}$$

Where: - $R_i$ = Total of row $i$. - $C_j$ = Total of column $j$. - $N$ = Grand total.

|               | Cured                    | Not cured                |
|---------------|--------------------------|--------------------------|
| **Treatment** | $(18 \times 20)/36 = 10$ | $(18 \times 20)/36 = 10$ |
| **Placebo**   | $(18 \times 16)/36 = 8$  | $(18 \times 16)/36 = 8$  |

### Statistic calculation

$\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$ where ($O_i$) are the observed frequencies, and ($E_i$) are the expected

### Code

```{r}
data_cure = data.frame(
  Outcome = factor(rep(c('Cured', 'Not cured'), e=18)),
  Group = factor(c(rep("Treatment", 12), rep("Placebo", 6), rep("Treatment", 8), rep("Placebo", 10))))

data_cure %>%  head()
```

### Contingency table

```{r}
contingency_table = table(data_cure$Outcome, data_cure$Group)
contingency_table
```

### Test

We don't need to calculate the expected values. The `chisq.test()` do it for us.

```{r}
chisq.test(contingency_table, 
           # correct = F
           )
```

-   $p.value > 0.05$

-   We **failed to reject** the $H_0$ about independence of `Outcome` and `Group`

# Fisher's Exact Test

## Assumptions

-   **for [both not random]{.underline} categories! (**otherwise it will be too conservative)

Usually used **only for 2x2 contingency table**

The test is **exact** because it computes the exact probability of the observed data (and more extreme data) under the null hypothesis

## Hypotheses

-   **Null Hypothesis (**$H_0$**):** There is [**no association**]{.underline} between two categorical variables
-   **Alternative Hypothesis (**$H_A$**):** There is [**some association**]{.underline} between two categorical variables

## Does the presence of the disease depend on the gene knockout?

### Data

|   | Expert guessed "treatment" | Expert guessed "control" | [*Row Total*]{.underline} |
|----|----|----|----|
| **Treatment** | [**1**]{.underline} | [**3**]{.underline} | 4 |
| **Control** | [**3**]{.underline} | [**1**]{.underline} | 4 |
| [***Column Total***]{.underline} | 4 | 4 | 8 |

```{r}
data_knockout = data.frame(
  Outcome = factor(rep(c('Disease', 'No disease'), e=4)),
  Group = factor(c("Knockout", "Control", rep("Knockout", 3), rep("Control", 3))))

data_knockout %>%  head()
```

### Contingency table

```{r}
contingency_table_1 = table(data_knockout$Group, data_knockout$Outcome)
contingency_table_1
```

### Testing

```{r fisher}
fisher.test(contingency_table_1)
```

-   $p.value > 0.05$
-   We failed reject $H_0$ about absence of association between categorical variables.

# P.S.

## Is your data normal?

### QQ-plot

Draws the correlation between a given sample and the normal distribution

### On sunlight plant data

Analyzing each group independantly

```{r}
qqnorm(data_sunlight_plant %>%  filter(group=='Sunlight') %>%  pull(height))
qqline(data_sunlight_plant %>%  filter(group=='Sunlight') %>%  pull(height))

qqnorm(data_sunlight_plant %>%  filter(group=='Shade') %>%  pull(height))
qqline(data_sunlight_plant %>%  filter(group=='Shade') %>%  pull(height))

```

### On blood pressure data

```{r}
qqnorm(data_pressure %>%  filter(group=='before') %>%  pull(pressure))
qqline(data_pressure %>%  filter(group=='before') %>%  pull(pressure))

qqnorm(data_pressure %>%  filter(group=='after') %>%  pull(pressure))
qqline(data_pressure %>%  filter(group=='after') %>%  pull(pressure))
```

## Chi-Square Goodness-of-Fit Test

## N x 1 contingency table

*Below are two different tables, but they are placed next to each other*

| Observed | Expected |
|----------|----------|
| 92       | 90       |
| 27       | 30       |
| 28       | 30       |
| 17       | 10       |

$\chi^2 = \sum^4_i \frac{(O_i - E_i)^2}{E_i}$, $df = N-1$

## Are observed ratios of pea plant traits consistent with Mendel's expected 9:3:3:1 ratio?

#### Data

```{r}
observed <- c(435, 150, 160, 55)
expected <- c(9, 3, 3, 1) / sum(c(9, 3, 3, 1)) * sum(observed)

```

```{r}
barplot(rbind(observed, expected), 
        beside = TRUE, 
        col = c("blue", "red"), 
        legend = c("Observed", "Expected"))
```

#### Code

```{r}
chisq.test(x=observed, p=expected/sum(expected))
```

-   `p` - expected proportions (instead of values)

-   $p.value > 0.05$ and it means that the two sets of frequencies are likely to belong to the same distribution or in other words: the observed values are not significantly different from the expected values

### 
